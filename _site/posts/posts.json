[
  {
    "path": "posts/2022-05-12-obtendo-coordenadas-a-partir-de-pdf/",
    "title": "Obtendo coordenadas a partir de PDF",
    "description": "Como obter dados de localização espacial a partir de texto.",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2022-05-12",
    "categories": [],
    "contents": "\r\nObtendo as coordenadas\r\nNo fim do último ano, recebi uma demanda para obter o perímetro das\r\náreas de uma Unidade de Conservação (UC) localizada na região Sul da\r\nBahia para verificação de possíveis sobreposições com locais a serem\r\nlicenciados.\r\nQuem lida com dados geoespaciais de UCs sabe que essa é uma tarefa\r\nrelativamente simples. Porém, também foi solicitado o perímetro da Zona\r\nde Amortecimento (ZA) desse local. Aí que tivemos uma complicação, pois\r\nnão pude achar nenhum material em formato vetorial com esse limite. A\r\nsaída foi obter o decreto\r\nde criação dessa UC e extrair os dados de posicionamento. E aí foi que o\r\nbarraco desabou: eram mais de duas centenas de coordenadas. Extrair essa\r\nquantidade manualmente seria complexo e foi então que decidi usar o\r\nR.\r\nRequisitos para extração de\r\ndados\r\nPara fazer essa obtenção, organização e plotagem de dados a partir do\r\ntexto do decreto em formato PDF foram necessárias algumas\r\nbibliotecas:\r\n\r\n\r\nlibrary(tidyverse) # conjunto de libs para manipulação de dados\r\nlibrary(here) # lib para definir tornar a obtenção de arquivos mias fácil\r\nlibrary(pdftools) # lib para ler o texto do .pdf\r\nlibrary(data.table) # lib para tabelar as coordenadas\r\nlibrary(sf) # lib para manipular arquivos vetorias\r\nlibrary(mapview) # lib para plotar os dados em um mapa interativo\r\n\r\n\r\n\r\nExtraindo os dados\r\nA aquisição de dados foi feita inicialmente com a leitura do arquivo\r\ne obtenção de 4 páginas de texto usando a lib {pdftools}:\r\n\r\n\r\ndecreto <- pdftools::pdf_text(here(\"decreto\", \"decreto.pdf\"))\r\n\r\n\r\n\r\nEm seguida, foi criada uma função get_coords usando\r\nexpressões regulares (regex) para obtenção de partes do texto que\r\nseguisse o seguinte padrão:\r\n1 - obter valores que começam com 8, seguido de uma série de números\r\ne possuem mais 6 digitos (lat)\r\n2 - obter valores que começam com 3 ou 4, seguido de uma série de\r\nnúmeros e possuem mais 5 digitos (long)\r\nA função ainda realiza a obtenção das coordenadas a partir de um\r\níndice (index) e as converte em valores numéricos, em\r\nseguida as salvando em uma coluna de tibble (tabela) que será\r\nunificada:\r\n\r\n\r\nget_coords <- function(pdf, index){\r\n  long <- tibble(long = str_extract_all(pdf, \"8[0-9]{6}\")[[index]] %>% as.numeric())\r\n  \r\n  lat <- tibble(lat = str_extract_all(pdf, \"[3-4][0-9]{5}\")[[index]] %>% as.numeric()) %>% \r\n    filter(lat != 409448 & lat != 400074)\r\n  \r\n  long %>% bind_cols(lat)\r\n  \r\n}\r\n\r\n\r\n\r\nEntão a função foi aplicada sobre as 4 páginas de texto obtidas\r\nusando a função map e as 4 tibbles obtidas então foram\r\nunificadas em uma única datatable com a função rbindlist.\r\nEm seguida, foi adicionada a coluna id contendo o número de\r\ncada linha da data.table.\r\nComo a leitura das páginas foi feita de forma geral, foi necessário\r\nidentificar qual base de dados correspondia às coordenadas da UC e qual\r\ncorrespondia às da ZA, para isso o documento foi avaliado e foi\r\nidentificada que a quebra acontecia na coordenada de número 224. Após a\r\nseparação das séries de coordenadas, foi percebido que um dos pontos do\r\nperímetro da Zona de Amortecimento tinha um erro de digitação. Então\r\nesse erro teve de ser corrigido de forma manual, sendo a coordenada\r\nadicionada à tabela de pontos:\r\n\r\n\r\n1:4 %>% \r\n  purrr::map(get_coords, pdf = decreto) %>%\r\n  data.table::rbindlist() -> my_df\r\n\r\nmy_df <- my_df %>% \r\n  mutate(id = 1:nrow(my_df))\r\n\r\nmy_df %>% filter(lat == 390724)\r\n\r\nmy_df_uc <- my_df %>% \r\n  filter(id < 224) %>% \r\n  select(-id)\r\n\r\nmy_df_za <- my_df %>% \r\n  filter(id >= 224) %>% \r\n  select(-id) %>% \r\n  bind_rows(c(long = 8181041, lat = 409448))\r\n\r\n\r\n\r\nPlotagem dos dados\r\nAgora vem a parte divertida da coisa: plotagem e análise visual.\r\nAssim, primeiro foi definido o sistema de projeção dos pontos a serem\r\nusados (Sirgas 2000 UTM Zone 24S). Em seguida as tabelas de pontos foram\r\nconvertidas em objetos sf (simple feature) para facilitar a\r\nsua manipulaçção e plotagem pela lib {sf}. Então, para comparar os\r\ndados, foi carregado o arquivo .shp oficial da UC em questão e feita a\r\nsua reprojeção espacial. Por fim, as três feições (pontos da UC, ZA e a\r\nárea oficial da UC) foram unificadas em uma lista e plotadas em um mapa\r\ninterativo usando a lib {mapview}:\r\n\r\n\r\nproj <- \"+proj=utm +zone=24 +south +ellps=aust_SA +towgs84=-57,1,-41,0,0,0,0 +units=m +no_defs\"\r\n\r\npoints_uc <- st_as_sf(x = my_df_uc, \r\n                   coords = c(\"lat\", \"long\"), \r\n                   crs = proj)\r\n\r\npoints_za <- st_as_sf(x = my_df_za, \r\n                      coords = c(\"lat\", \"long\"), \r\n                      crs = proj)\r\n\r\narea_uc <- read_sf(here(\"ucs\", \"ucstodas.shp\")) %>% \r\n  filter(NOME_UC1 == \"PARQUE NACIONAL DO ALTO CARIRI\") %>% \r\n  st_transform(crs = st_crs(proj))\r\n\r\nmapview(list(points_uc, points_za, area_uc), color = c(\"red\", \"green\", \"white\"))\r\n\r\n\r\n\r\nÁreas do Parque Nacional do Alto Cariri,\r\nna Bahia.Conclusão\r\nEssa foi uma abordagem simples e que nos poupou bastante tempo.\r\nEspero que este materail possa servir de apoio a pessoas que encontrem\r\nos mesmos desafios que tive. Até a próxima!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-12-obtendo-coordenadas-a-partir-de-pdf/featured.jpg",
    "last_modified": "2022-05-12T22:32:27-03:00",
    "input_file": "obtendo-coordenadas-a-partir-de-pdf.knit.md"
  },
  {
    "path": "posts/2021-11-22-logging-Python/",
    "title": "Como usar logging em Python",
    "description": "Uma breve introdução ao uso de logging.",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2022-02-04",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Basemap com o local das coordendas informadas.\r\n\r\n\r\n\r\nPor qual razão usar logging?\r\nO uso de logging visa facilitar o monitoramento de eventos durante a execução de um programa. Isso pode ser feito a partir da expressão print(). Porém o módulo logging fornece capabilidades mais elegantes de rastrear os eventos dentro do programa em questão. Isso é feito adicionando chamadas de logging ao longo do código. Achou a coisa meio abstrata? Vamo ver na prática então.\r\nExemplo prático\r\nApenas recentemente tive contato com o módulo logging nos meus estudos de Python. Isso aconteceu quando estava realizando o desenvolvimento de um código pra utilizar no meu trabalho na área de geoprocessamento. Neste caso, vou mostrar como usar logging para apresentar os acontecimentos durante a criação de um basemap com os pontos do meu interesse. A seguir são importadas as libs necessárias:\r\n\r\nimport pandas as pd\r\nimport plotly.express as px\r\nimport utm\r\n\r\nimport logging\r\n\r\nA primeira coisa a ser feita é definir as configurações de logging. Neste caso, optei por definir a formatação do output para apresentar o nível de severidade dos eventos (neste caso, o mais baixo logging.DEBUG) e a mensagem definida na chamada. Aqui eventos de qualquer nível pode ser rastreado, podendo ser definido a escolha do desenvolvedor entre 5 níveis de severidade distintos (DEBUG, INFO, WARNING, ERROR e CRITICAL).\r\n\r\nlogging.basicConfig(format='%(levelname)s - %(message)s', level=logging.DEBUG)\r\n\r\nEm seguida, dei sequência ao desenvolvimento do programa. Neste caso, defini o Descriptor Protocol Coordinate para gerenciar as coordenadas geodésicas passadas como atributos à classe Point. Como coordendas são em pares (x, y), uma forma de evitar repetir o código de definição de getters e setters de ambos os valores foi a partir do uso de descriptors:\r\n\r\nclass Coordinate:\r\n    def __set_name__(self, owner, name):\r\n        self._name = name\r\n\r\n    def __get__(self, instance, owner):\r\n        return instance.__dict__[self._name]\r\n\r\n    def __set__(self, instance, value):\r\n        try:\r\n            instance.__dict__[self._name] = float(value)\r\n            logging.info('Validated values.')\r\n        except ValueError:\r\n            logging.exception(f'\"{self._name}\" must be a number')\r\n\r\nNeste caso, utilizei logging para monitorar se os valores de x e y passados ao descriptor Coordinate são valores válidos. Caso estes valores não sejam válidos, é levantado um erro e uma mensagem de logging. Mais sobre descriptors pode ser acessado aqui. Pronto! Esses foram os primeiros casos de uso de logging.\r\nEm seguida, defin a classe Point, em que a mesma recebe informações sobre coordenadas de um ponto em específico:\r\n\r\nclass Point:\r\n    x = Coordinate()\r\n    y = Coordinate()\r\n\r\n    def __init__(self, x, y, zone=None, northern=None):\r\n        self.x = x\r\n        self.y = y\r\n        self.zone = zone\r\n        self.northern = northern\r\n\r\n    def __str__(self):\r\n        if self.zone is not None and self.northern is True:\r\n            return f'Long: {self.x}m, Lat: {self.y}m, Zone: {self.zone}N'\r\n        elif self.zone is not None and self.northern is False:\r\n            return f'Long: {self.x}m, Lat: {self.y}m, Zone: {self.zone}S'\r\n        else:\r\n            return f'Lat: {self.x}°, Long: {self.y}°'\r\n\r\n    def __repr__(self):\r\n        if self.zone is not None and self.northern is True:\r\n            return f'Point({self.x}, {self.y}, {self.zone}, northern={self.northern})'\r\n        elif self.zone is not None and self.northern is False:\r\n            return f'Point({self.x}, {self.y}, {self.zone}, northern={self.northern})'\r\n        else:\r\n            return f'Point({self.x}, {self.y})'\r\n    \r\n    def plot_coord(self):        \r\n        if self.zone is None:\r\n            lat, long = self.x, self.y            \r\n        else:\r\n            logging.warning('Converting coordinates to UTM format.')\r\n            lat, long = utm.to_latlon(self.x, self.y, self.zone, northern=self.northern)\r\n            logging.warning('Conversion is completed.')\r\n            \r\n            \r\n        fig = px.scatter_mapbox(lat=pd.Series([lat]), lon=pd.Series([long]),\r\n                            color_discrete_sequence=[\"fuchsia\"], zoom=3, height=300)\r\n        fig.update_layout(mapbox_style=\"open-street-map\")\r\n        fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\r\n        fig.show()\r\n\r\nNesta classe primeiro são chamados os descriptos para compartilhar o comportamento das diferentes coordendas (podendo ser adicionada uma terceira z). Então a instância da classe é inicializada com os valores das coordendas (x e y) e com informações sobre a zona e o hemisfério. Estas duas últimas são importantes em situações onde os valores das coordenadas informadas são em UTM. Assim, pode-se ter duas formas de passar dos valores destes atributos.\r\nO próximo passo foi definir as formas de representação da classe com os dunder methods repr e str, também com validação de modo a apresentar o formato correto das coordenadas ao chamar uma instância da classe Point.\r\nE, por fim, é definido um método para plotar um basemap interativo com a localização do ponto em questão. O basemap é oriundo da lib plotly e recebe apenas valores em formato geodésico, sendo necessária a conversão de coordendas quando passadas em formato UTM. Antes de realizar a conversão usando a lib utm, são apresentados logging.warnings de que está sendo feita uma conversão das coordendas e que a conversão foi finalizada.\r\nTestando os resultados\r\nAgora resta realizar os testes e avaliar o uso da lib logging com coordendas em formato UTM:\r\n\r\n>>> utm_coords = Point(283979.44, 8451361.31, 24, False)\r\nINFO - Validated values.\r\nINFO - Validated values.\r\n\r\n>>> utm_coords\r\nPoint(283979.44, 8451361.31, 24, northern=False)\r\n\r\n>>> print(utm_coords)\r\nLong: 283979.44m, Lat: 8451361.31m, Zone: 24S\r\n\r\n>>> utm_coords.plot_coord() # O basemap é gerado\r\nWARNING - Converting coordinates to UTM format.\r\nWARNING - Conversion is completed.\r\n\r\nTestes com coordendas em formato geodésico:\r\n\r\n>>> geo_coords = Point(-14, -41)\r\nINFO - Validated values.\r\nINFO - Validated values.\r\n\r\n>>> geo_coords\r\nPoint(-14.0, -41.0) \r\n\r\n>>> print(geo_coords)      \r\nLat: -14.0°, Long: -41.0°  \r\n\r\n>>> geo_coords.plot_coord() # O basemap é gerado\r\n\r\nTestes com coordendas em formato “geodésico”errado”:\r\n\r\n>>> geo_coords.x = 'a'\r\nERROR - \"x\" must be a number\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 8, in __set__\r\nValueError: could not convert string to float: 'a'\r\n\r\nConclusão\r\nO uso de logging se mostrou bastante útil no desenvolvimento desse material, sendo uma das coisas mais legais que estudei até agora (a paixão por Python só aumenta). Muito obrigado pela visita e espero que tenham gostado da leitura!\r\nMais sobre logging pode ser encontrado nestas referências:https://realpython.com/python-logging/ https://www.youtube.com/watch?v=-ARI4Cz-awo&ab_channel=CoreySchafer (um dos melhores canais de Python)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-22-logging-Python/featured.png",
    "last_modified": "2022-02-04T00:05:05-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-20-predizendo-volume-de-eucalipto-com-tidymodels-xgboost-e-targets/",
    "title": "Predizendo volume de eucalipto com tidymodels, XGBoost e targets",
    "description": "\"Como montar um ambiente reprodutível para o ajuste de modelos de predição do volume de eucalipto.\"",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2021-10-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Workflow do trabalho.\r\n\r\n\r\n\r\nUmas das dificuldades encontradas na academia é a reprodutibilidade de trabalhos. Essa é uma situação que, infelizmente, é corriqueira na área florestal. Neste texto, apresento algumas ferramentas que buscam melhorar a forma que trabalhos podem ser replicados, como a utilização dos pacotes here e renv.\r\nAproveitando o embalo, também mostro como usar o pacote tidymodels para a predição do volume de eucalipto. O dataset utilizado neste projeto é oriundo do excelente trabalho publicado no ano de 2020 pelo professor Gileno Azevedo juntamente com outros pesquisadores. O dataset pode ser encontrado na página do artigo. Vamos lá!\r\n1ª etapa - Iniciando o projeto\r\nAntes de qualquer coisa, preciso falar de 3 ferramentas básicas quando se busca reprodutibilidade em trabalhos no R:\r\nrenv\r\nEste pacote é um gerenciador de dependências: ele organiza e “memoriza” as dependências (como pacotes) que o seu projeto está usando de modo que, caso alguém refaça suas análises, não ocorra problemas como o uso de pacotes de versões distintas, além de tornar seu ambiente de trabalho isolado. Mais à frente explicarei outra importante vantagem. Caso queira saber mais sobre o renv, esse tutorial do Chaoran é show.\r\nR Projects\r\nÉ o mais simples dessa lista. Basicamente toda vez que você for criar um projeto, o ideal é que seja criada uma pasta principal e dentro dela serão criadas as demais pastas (data, R, etc.). Em seguida, abra o RStudio e crie um Project dentro desta pasta principal. A organização destas pastas terá a seguinte estrutura (a mesma utilizada neste trabalho):\r\n\r\n\r\n# Eucalipto_Volume (pasta principal)  \r\n#      |--Eucalipto_Volume.Rproj\r\n#      |--R  \r\n#      |--plots\r\n#      |--report\r\n\r\n\r\n\r\nhere\r\nEsse aqui é o mais legal: basicamente ele te permite criar caminhos relativos. Por ex.: ao invés de se referir a um arquivo como dados <- C:user/zezin/uma/duas/dados/meus_dados.csv, ou usar setwd(\"C:user/zezin/uma/duas/dados\"), prefira salvar seu Project em uma pasta principal e se referir a ele como here::here(\"dados\", \"meus_dados.csv\"). Isso facilita que outras pessoas reproduza suas análises sem grandes problemas. Tem um pessoal que recomenda fortemente usar o pacote here. Basicamente ele define o caminho de acesso aos arquivos a partir da última pasta do caminho (top-level folder), que no nosso exemplo hipotético é a pasta “dados”.\r\nMão na massa\r\nA primeira coisa que fiz foi criar uma pasta principal. Em seguida, criei dentro dela um R Project com o nome do meu projeto (Eucalipto_Volume). Então instalei o renv e executei o comando renv::init() para criar um ambiente local do projeto. A partir daí, iniciei as análises e instalei os pacotes básicos iniciais necessários (incluindo o here). Sempre que instalava um novo pacote, chamava renv::snapshot() para atualizar os estado do projeto e as dependências. Caso fosse necessário reverter alguma alteração de dependência no projeto após chamar renv::snapshot(), era acionado o comando renv::restore(). Pronto! Meu ambiente local do projeto está montado e isolado. Bacana, né?\r\n2ª etapa - Modelagem\r\nAgora a parte legal: a modelagem dos dados! Recentemente tem ganhado bastante tração um pacote de modelagem do R baseado na filosofia do tidyverse: o tidymodels. Esse pacote é incrível, permitindo usar diversos modelos de machine learning de forma extremamente intuitiva e bem elegante. A seguir vou mostrar o passo a passo necessário para ajustar e testar diversos modelos visando a predição do volume de eucalipto.\r\nCarregando os pacotes necessários:\r\n\r\n\r\nlibrary(here)\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\nlibrary(janitor)\r\nlibrary(EnvStats)\r\nlibrary(bestNormalize)\r\nlibrary(finetune)\r\nlibrary(doParallel)\r\nlibrary(extrafont)\r\n\r\n\r\n\r\nAquisição dos dados\r\nPrimeiramente serão adquiridas as bases de dados (treino e teste) a serem utilizadas no projeto a partir do site que disponibiliza o artigo e os dados. Aqui será criada uma pasta “data” onde os arquivos da base de dados serão baixados.\r\n\r\n\r\n# Criando o diretório onde a base será alocada\r\ndir.create(\"data\")\r\n\r\n# Acesso ao material\r\nlink_train <- \"https://doi.org/10.1371/journal.pone.0238703.s007\"\r\nlink_test <- \"https://doi.org/10.1371/journal.pone.0238703.s008\"\r\n\r\n# Aqui é criado o caminho de acesso às bases de dados\r\ndest_folder_train <- here::here(\"data\", \"train.xlsx\")\r\ndest_folder_test <- here::here(\"data\", \"test.xlsx\")\r\n\r\n# Aquisição da base de teste e treino usadas pelos autores do trabalho\r\nutils::download.file(link_train, \r\n                     destfile = dest_folder_train, \r\n                     mode = \"wb\")\r\n\r\nutils::download.file(link_test, \r\n                     destfile = dest_folder_test, \r\n                     mode = \"wb\")\r\n\r\n\r\n\r\nPerceba que aqui é usado o pacote here para a criação de um caminho relativo até a pasta de destino, em que o caminho até a pasta objetivo (neste caso, a pasta “data”) se inicia a partir da pasta principal criada para o projeto (Eucalipto_Volume), e não do seu diretório base. Caso a pasta principal do projeto seja alterada para outra partição, não será necessário chamar setwd() com um novo caminho.\r\nE qual a vantagem disso? Isso permite que você ou qualquer outra pessoa não precise alterar o caminho caso queira carregar/salvar as informações obtidas das mesmas análises aqui apresentadas. A utilização desse pacote é uma boa prática quando falamos de project-oriented workflows (workflow montado tendo como base projects).\r\nPreparando os dados\r\nAs bases de dados foram separadas originalmente pelos autores em dois arquivos. Essa é uma boa prática para criação de modelos e sua subsequente avaliação. Dividir os sets dessa forma permite que não ocorra data leakage (quando seu modelo, na fase de treino, “entra em contato” com os dados de teste, causando overfitting). Entretanto, uma vantagem do tidymodels é que ele permite o split entre set de treino e set de teste de forma unificada, sendo geradas partições de um mesmo arquivo sem a ocorrência de data leakage. Para ilustrar essa capacidade, os datasets serão unidos e então será mostrado como realizar a sua separação em treino e teste com o tidymodels:\r\n\r\n\r\n# Carregando a base de daoos separada\r\nsheet_train <- readxl::read_excel(here::here(\"data\", \"train.xlsx\"), \r\n                                  skip = 1)\r\nsheet_test <- readxl::read_excel(here::here(\"data\", \"test.xlsx\"), \r\n                                 skip = 1)\r\n\r\n# Unificando a base e \"limpando os nomes\" das colunas\r\ndataset <- \r\n    dplyr::bind_rows(sheet_train, sheet_test) |>\r\n    janitor::clean_names() |> \r\n    dplyr::mutate(across(c(stem:rotation), \r\n                         forcats::as_factor),\r\n                  across(c(tx, d), as.integer))\r\n\r\n\r\n\r\nIniciando o processo de modelagem\r\nApós unificar os dados, vamos colocar a mão na massa! A primeira coisa a ser feita é dividir os dados em treino e teste e então realizar a reamostragem repetida do set de treino para validação dos modelos. O tidymodels faz isso de forma prática:\r\n\r\n\r\nset.seed(1504)\r\n\r\nvol_split <- rsample::initial_split(data = dataset, \r\n                                    prop = 0.75, \r\n                                    strata = cod_volume)\r\ntrain_split <- rsample::training(vol_split)\r\ntest_split <- rsample::testing(vol_split)\r\n\r\n# Realizando validação cruzada repetida para avaliação dos modelos\r\nvol_folds <- rsample::vfold_cv(data = train_split, \r\n                               strata = cod_volume, \r\n                               repeats = 5)\r\n\r\n\r\n\r\nAlgumas considerações nessa etapa:\r\nO split inicial foi realizado usando uma proporção de 3/4.\r\nA divisão foi estratificada de acordo com as classes de volume comercial.\r\nO tidymodels permite a divisão do dataset de forma simples e que não ocorra data leakage, dispensando a necessidade de separar os dados manualmente.\r\nExplorando os dados\r\nNeste caso, temos um set de dados com poucos preditores: dbh (DAP) e h (Altura). Neste dataset também constam as variáveis tx e d que dizem respeito a se o volume de madeira predito é com casca ou sem casca (0 ou 1) e o diâmetro comercial, respectivamente. De modo a observar os padrões dos dados, vamos fazer uma análise exploratória básica das variáveis:\r\n\r\n\r\nggplot(data = train_split) +\r\n    geom_density(mapping = aes(x = dbh)) +\r\n    labs(x = \"DAP\")\r\n\r\nggplot(data = train_split) +\r\n    geom_density(mapping = aes(x = h)) +\r\n    labs(x = \"Altura\")\r\n\r\n# Explorando as relações entre as variáveis principais\r\nggplot(data = train_split) +\r\n    stat_count(mapping = aes(x = cod_volume))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = dbh, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = dbh^2, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = h^2, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = h, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = dbh*h, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = (dbh*h)^2, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = dbh, y = h))\r\n\r\n\r\n\r\nNeste caso, é recomendável normalizar os dados antes de seguir com a modelagem. Sem a padronização, uma variável pode ter um maior impacto sobre a resposta apenas por conta da sua escala, o que pode ser o caso aqui dadas as próprias unidades (cm e m) dos dados Essa normalização será aplicada mais à frente na modelagem.\r\nPode-se perceber também que a adição de interações e de termos quadráticos tornam a relação entre os preditores, especificamente dap e h, mais linear.\r\nTambém é possível observar que as classes de volume não possuem a mesma quantidade de observações. Neste caso, pode-se seguir com duas estratégias: a primeira seria ajustar um modelo para cada um dos diferentes volumes em separado; a segunda consiste na divisão ponderada (estratificação) no split dos dados (o treino e o teste teriam a mesma proporção de cada classe de volume). Neste trabalho, seguiremos a segunda estratégia.\r\nAjustando os modelos\r\nAgora vem a parte onde a mágica do tidymodels acontece: Primeiro é definida uma recipe do passo-a-passo que os dados devem ser processados. Em seguida são definidas as especificações modelos a serem utilizados. Serão definidas duas recipes (receitas de passo-a-passo): uma sem pré-processamento e outra pré-processada (com adião de interações, termos quadráticos e normalização):\r\n\r\n\r\n# Definindo os pré-processamentos dos dados\r\nsimple_vol_rec <- recipes::recipe(v ~ dbh + h + tx + d, data = train_split)\r\n\r\nnormalized_vol_rec <- simple_vol_rec |> \r\n  recipes::step_interact(terms = ~ dbh:h) |>  \r\n  recipes::step_mutate(dbh_sqrd = dbh^2, \r\n                       h_sqrd = h^2) |> \r\n  recipes::step_normalize(recipes::all_predictors(), -tx, -d)\r\n\r\n\r\n\r\nAqui pode-se perceber que a recipe normalized_vol_rec é apenas a recipe simple_vol_rec com mais “passos” adicionados.\r\nEm seguida, são definidos os diversos modelos a serem ajustados e depois testados. Estes modelos terão os hiperparâmetros mais importantes “marcados” para tunagem usando a função tune::tune.\r\n\r\n\r\n# Definindo diversos modelos\r\nlm_mod <- \r\n     parsnip::linear_reg() |> \r\n     parsnip::set_engine(\"lm\") |> \r\n     parsnip::set_mode(\"regression\")\r\n  \r\npenalized_lm_mod <- \r\n  parsnip::linear_reg(penalty = tune::tune(),\r\n                      mixture = tune::tune()) |> \r\n  parsnip::set_engine(\"glmnet\") |>\r\n  parsnip::set_mode(\"regression\")\r\n\r\nbag_mars_mod <- \r\n  baguette::bag_mars(prod_degree = tune::tune(), \r\n                     prune_method = \"exhaustive\",\r\n                     num_terms = tune::tune()) |> \r\n  parsnip::set_engine(\"earth\", times = 4) |> \r\n  parsnip::set_mode(\"regression\")\r\n  \r\ndec_tree <- \r\n  parsnip::decision_tree(cost_complexity = tune::tune(),\r\n                         tree_depth = tune::tune(),\r\n                         min_n = tune::tune()) |> \r\n  parsnip::set_engine(\"rpart\") |> \r\n  parsnip::set_mode(\"regression\")\r\n\r\nbag_cart_mod <- \r\n  baguette::bag_tree(cost_complexity = tune::tune(),\r\n                     tree_depth = tune::tune(),\r\n                     min_n = tune::tune()) |> \r\n  parsnip::set_engine(\"rpart\", times = 50L) |>\r\n  parsnip::set_mode(\"regression\")\r\n\r\nrf_spec <- \r\n  parsnip::rand_forest(mtry = tune::tune(), \r\n                       min_n = tune::tune(),\r\n                       trees = 1000) |> \r\n  parsnip::set_engine(\"ranger\") |> \r\n  parsnip::set_mode(\"regression\")\r\n  \r\nxgb_spec <- \r\n  parsnip::boost_tree(tree_depth = tune::tune(),\r\n                      learn_rate = tune::tune(),\r\n                      loss_reduction = tune::tune(),\r\n                      min_n = tune::tune(),\r\n                      sample_size = tune::tune(),\r\n                      mtry = tune::tune(),\r\n                      trees = 1000,\r\n                      stop_iter = 20) |> \r\n  parsnip::set_engine(\"xgboost\") |> \r\n  parsnip::set_mode(\"regression\")\r\n  \r\nsvm_r_spec <- \r\n  parsnip::svm_rbf(cost = tune::tune(),\r\n                   rbf_sigma = tune::tune(),\r\n                   margin = tune::tune()) |> \r\n    parsnip::set_engine(\"kernlab\") |> \r\n    parsnip::set_mode(\"regression\")\r\n  \r\nnnet_spec <- \r\n  parsnip::mlp(hidden_units = tune::tune(),\r\n               penalty = tune::tune(), \r\n               epochs = tune::tune()) |> \r\n    parsnip::set_engine(\"nnet\") |> \r\n    parsnip::set_mode(\"regression\")\r\n  \r\n  \r\nspecs_vol <- list(\"linear_reg\" = lm_mod, \r\n                  \"bag_mars\"= bag_mars_mod,\r\n                  \"decision_tree\" = dec_tree, \r\n                  \"bag_cart\" = bag_cart_mod, \r\n                  \"rf\" = rf_spec, \r\n                  \"xgb\" = xgb_spec,\r\n                  \"svm_rbf\" = svm_r_spec, \r\n                  \"nnet_mlp\" = nnet_spec,\r\n                  \"penalized_reg\" = penalized_lm_mod)\r\n\r\n\r\n\r\nTodas as especificações foram alocadas em uma lista que será usada mais à frente. Até aqui temos as duas recipes de pré-processamento a serem usadas nos dados e os 9 modelos especificados a serem tunados/ajustados.\r\nAgora vem a pergunta: como juntar tudo isso?\r\nEm um workflow (ou um workflowset, melhor dizendo).\r\n\r\n\r\n# Definindo o workflowset (serão ajustados 18 modelos ao todo)\r\nwflow_vol <- workflowsets::workflow_set(\r\n  models = specs_vol, #lista de modelos\r\n  preproc = list(\r\n    normalized = normalized_vol_rec,\r\n    simple = simple_vol_rec)\r\n  )\r\n\r\n\r\n\r\nCriado o workflowset, então será definida a configuração e o tipo de tunagem a ser aplicada. Neste caso, o processo de tunagem usado (race tuning) avalia todos os modelos em um set inicial da reamostragem. Baseado na performance corrente das métricas, alguns parâmetros que produzem modelos ruins, do ponto de vista preditivo, são então descartados na sequência do processo. Mais sobre esse processo de tunagem pode ser encontrado no livro do tidymodels.\r\n\r\n\r\n# Definindo as configurações da tunagem de parâmetros usando computação paralela\r\nracing_ctrl <- finetune::control_race(\r\n  save_pred = TRUE,\r\n  parallel_over = \"everything\",\r\n  save_workflow = TRUE\r\n)\r\n\r\n\r\n# Definindo a execução em paralelo \r\nclusters <- parallel::detectCores()\r\ncl <- parallel::makePSOCKcluster(clusters)\r\ndoParallel::registerDoParallel(cl)\r\n\r\n# Realizando a tunagem dos modelos\r\nresults_vol <-\r\n  workflowsets::workflow_map(\r\n    wflow_vol,\r\n    seed = 1504,\r\n    resamples = vol_folds,\r\n    control = racing_ctrl,\r\n    fn = \"tune_race_anova\",\r\n    grid = 25,\r\n    metrics = yardstick::metric_set(yardstick::rmse,\r\n                                    yardstick::rsq, \r\n                                    yardstick::huber_loss, \r\n                                    yardstick::mae)\r\n)\r\n# Parando a execução em paralelo \r\nparallel::stopCluster(cl)\r\nforeach::registerDoSEQ()\r\n\r\n\r\n\r\nApós realização da tunagem e escolhida a melhor configuração de parâmetros dos modelos de melhor desempenho, é feita a avaliação usando gráficos de acordo com as métricas definidas:\r\n\r\n\r\n# Plotagem dos resultados para cada métrica utilizada\r\nplot_results <- function(race_rslts, mtrc = \"rmse\",...){\r\n  workflowsets::autoplot(\r\n    race_rslts,\r\n    rank_metric = mtrc,  \r\n    metric = mtrc,       \r\n    select_best = TRUE,\r\n    ...\r\n    ) -> plot_racing\r\n  \r\n  ggplot2::ggsave(glue::glue(mtrc, \".png\"), \r\n                  path = here::here(\"plots\"))\r\n  graphics::plot(plot_racing)\r\n  \r\n}\r\n\r\nplot_results(results_vol, mtrc = \"rsq\")\r\nplot_results(results_vol, mtrc = \"rmse\")\r\nplot_results(results_vol, mtrc = \"huber_loss\")\r\nplot_results(results_vol, mtrc = \"mae\")\r\n\r\n\r\n\r\nAvaliação da qualidade dos modelos\r\nEm seguida, deve ser realizada a seleção do melhor modelo. Para isso, são criadas funções de:\r\nSeleção do melhor modelo.\r\nAjuste final do modelo selecionado.\r\nObtenção das métricas de avaliação do modelo selecionado no set de teste.\r\n\r\n\r\n# Função para seleção de modelos de acordo com o R² (padrão)\r\nselect_models <- function(grid_results, metric = \"rsq\", rank_posit = 1){\r\n  \r\n  workflowsets::rank_results(grid_results, \r\n                             select_best = TRUE) |> \r\n  dplyr::relocate(rank) |> \r\n  dplyr::select(-c(.config, model, std_err)) |> \r\n  dplyr::filter(.metric == \"rsq\" & rank == rank_posit) -> model_selected\r\n  EnvStats::print(model_selected)\r\n  \r\n}\r\n\r\n# Função para ajuste final do modelo\r\nfit_model <- function(grid_results, model_ranked, df_split, metric = \"rmse\"){\r\n    \r\n    name_model <- model_ranked |> purrr::pluck(\"wflow_id\", 1)\r\n  \r\n    model <- grid_results |> \r\n        workflowsets::extract_workflow_set_result(id = name_model) |>\r\n        tune::select_best(metric = metric)\r\n    \r\n    grid_results |> \r\n        workflowsets::extract_workflow(name_model) |>\r\n        tune::finalize_workflow(model) |> \r\n        tune::last_fit(split = df_split,\r\n                       metrics = yardstick::metric_set(yardstick::rmse, \r\n                                                       yardstick::rsq, \r\n                                                       yardstick::huber_loss, \r\n                                                       yardstick::mae))\r\n    \r\n}\r\n\r\n# Função para obtenção da performance do melhor modelo no set de teste\r\nmetrics_mod <- function(best_mod, model_ranked){\r\n  \r\n  name_model <- model_ranked |> purrr::pluck(\"wflow_id\", 1)\r\n  \r\n  workflowsets::collect_metrics(best_mod) |> \r\n    kableExtra::kbl(caption = glue::glue(\"Performance do modelo \", \r\n                                         name_model)) |> \r\n    kableExtra::kable_classic(full_width = F, \r\n                              html_font = \"Cambria\", \r\n                              font_size = 16) |> \r\n    kableExtra::save_kable(file = here::here(\"plots\", \r\n                                             glue::glue(\"perf_\", \r\n                                                        name_model, \r\n                                                        \".png\")),\r\n                           self_contained = T)\r\n  \r\n  workflowsets::collect_metrics(best_mod)\r\n}\r\n\r\n# Seleção, ajuste e avaliação do melhor modelo\r\nbest_model <- select_models(grid_results = results_vol)\r\n\r\nfit_best_mod <- fit_model(grid_results = results_vol,\r\n                        model_ranked = best_model,\r\n                        df_split = vol_split)\r\n\r\nmetrics_mod(fit_best_mod, best_model)\r\n\r\n\r\n\r\nFigure 2 - Desempenho dos modelos.De modo a termos um ponto de comparação didático, serão comparados um modelo de qualidade intermediária e o melhor modelo. Esse processo se dá de forma similar ao realizado previamente, exceto que agora será selecionado o modelo ranqueado na posição número 10 (rank_posit = 1) em contraste ao melhor modelo (rank_posit = 1):\r\n\r\n\r\n# Seleção, ajuste e avaliação do modelo intermediário\r\nintermediary_model <- select_models(grid_results = results_vol, rank_posit = 10)\r\n\r\nfit_intermediary_model <- fit_model(grid_results = results_vol,\r\n                                       model_ranked = intermediary_model,\r\n                                       df_split = vol_split)\r\n\r\nmetrics_mod(fit_intermediary_model, intermediary_model)\r\n\r\n\r\n\r\nAgora ambos os modelos ajustados previamente serão comparados através de gráficos de dispersão contendo o volume predito vs volume observado, permitindo uma análise visual da qualidade preditiva de cada um. Para isso serão definidas duas funções para geração do gráfico de dispersão de cada modelo.\r\n\r\n\r\n# Função para plotar os resultados dos modelos\r\nscatterplot <- function(.x, .y){\r\n    \r\n    ggplot2::ggplot(.x) + \r\n        ggplot2::geom_point(mapping = ggplot2::aes(x = .pred, y = v)) + \r\n        ggplot2::geom_point(mapping = ggplot2::aes(x = .pred, y = v), \r\n                   alpha = 0.5, \r\n                   size = 2, \r\n                   color = \"black\", \r\n                   fill = \"mediumspringgreen\", \r\n                   pch = 21) + \r\n        ggplot2::geom_abline(lty = 2, \r\n                    col = \"black\", \r\n                    size = 1) +\r\n        tune::coord_obs_pred() +\r\n        ggplot2::theme_classic() +\r\n        ggplot2::theme(text = ggplot2::element_text(family = \"Source Code Pro\")) +\r\n        ggplot2::ggtitle(.y) +\r\n        ggplot2::labs(x = \"Volume Predito (m³)\",\r\n                      y = \"Volume Observado (m³)\") -> plot_scatterplot\r\n    \r\n  ggplot2::ggsave(glue::glue(.y, \".png\"), path = here::here(\"plots\"))\r\n  graphics::plot(plot_scatterplot)\r\n  \r\n}\r\n\r\n# Função para extrair os dados preditos e plotá-los vs os \r\n# dados observados usando a função anterior\r\naccessing_models <- function(mod1, mod2, model_ranked1, model_ranked2){\r\n    \r\n    gsub_und <- function(x) gsub(\"_\", \" \", x)\r\n    name_model1 <- model_ranked1 |> purrr::pluck(\"wflow_id\", 1) |> gsub_und()\r\n    name_model2 <- model_ranked2 |> purrr::pluck(\"wflow_id\", 1) |> gsub_und()\r\n    \r\n    workflowsets::collect_predictions(mod1) |> \r\n        dplyr::mutate(model = name_model1) |> \r\n        dplyr::bind_rows(workflowsets::collect_predictions(mod2) |> \r\n                      dplyr::mutate(model = name_model2)) |>\r\n        tidyr::nest(data = -c(model)) |> \r\n        dplyr::mutate(plots = map2(data, model, scatterplot))\r\n    \r\n}\r\n\r\naccessing_models(fit_best_mod, \r\n                 fit_intermediary_model, \r\n                 best_model, \r\n                 intermediary_model)\r\n\r\n\r\n\r\nFigure 3 - Volume observado vs volume predito usando o modelo XGBoost.Conclusão da modelagem\r\nÉ possível observar que o modelo XGBoost com pré-processamento normalizado apresentou um desempenho elevado frente aos demais modelos, apontando a sua aplicabilidade para a predição de volume de madeira de eucalipto em florestas plantadas. É digno de nota que, de forma similar ao que foi apontado por Azevedo et al. (2020), o modelo de redes neurais artificiais (MLP) apresentou um R² de ~0,96.\r\n3ª etapa - Reprodutibilidade\r\nAgora vamos para etapa que foca no uso do pacote targets. Com o targets pode ser mantido uma rotina de trabalho reprodutível que evita ao máximo repetições. O pacote “aprende” como o seu fluxo de trabalho se encaixa e pula a execução de tarefas pesadas que já estão atualizadas. Desse modo, ele executa apenas as etapas necessárias ou que foram alteradas/adicionadas. Mais informações sobre esse pacote podem ser encontradas em seu manual.\r\nE como o targets funciona?\r\nBasicamente ele depende de funções:\r\nPara todas as etapas da modelagem que foi realizada foi criada uma função que foi alocada em um script (functions.R), assim, o fluxo de trabalho é subdividido e pode rodar de forma mais independente. Lembra da pasta “Eucalipto_Volume/R” criada lá no começo? Então, dentro dessa pasta é onde está o script com as funções definidas. Esse script pode ser acessado aqui. Um exemplo de função criada para a modelagem foi a da criação da base de dados na pasta “data”:\r\n\r\n\r\nreadind_data <- function(){\r\n    \r\n    dir.create(\"data\")\r\n    \r\n    link_train <- \"https://doi.org/10.1371/journal.pone.0238703.s007\"\r\n    link_test <- \"https://doi.org/10.1371/journal.pone.0238703.s008\"\r\n    \r\n    dest_folder_train <- here::here(\"data\", \"train.xlsx\")\r\n    dest_folder_test <- here::here(\"data\", \"test.xlsx\")\r\n    \r\n    utils::download.file(link_train, \r\n                         destfile = dest_folder_train, \r\n                         mode = \"wb\") # wb se OS for Windows\r\n    utils::download.file(link_test, \r\n                         destfile = dest_folder_test, \r\n                         mode = \"wb\")\r\n    \r\n    sheet_train <- readxl::read_excel(dest_folder_train, skip = 1)\r\n    sheet_test <- readxl::read_excel(dest_folder_test, skip = 1)\r\n    \r\n    dplyr::bind_rows(sheet_train, sheet_test) |> \r\n        janitor::clean_names() |>  \r\n        dplyr::mutate(across(c(stem:rotation), \r\n                             forcats::as_factor),\r\n                      across(c(tx, d), as.integer))\r\n    \r\n}\r\n\r\n\r\n\r\nAs demais funções seguem esse mesmo estilo.\r\nUsando o targets\r\nÉ agora que o targets entra na brincadeira de vez. Para orquestrar o fluxo de trabalho primeiramente deve ser criado um script (_targets.R) alocado dentro da pasta principal (“Eucalipto_Volume”). Neste script são carregados os pacotes targets e tarchetypes (falo dele mais à frente), também é deve ser carregado o script com as funções definidas na etapa anterior e alocado na pasta “R”, a partir da função base::source() (perceba o uso do pacote here):\r\n\r\n\r\nlibrary(targets)\r\nlibrary(tarchetypes)\r\nsource(here::here(\"R\", \"functions.R\"))\r\n\r\n\r\n\r\nEm seguida, deve ser criada uma lista tendo os targets como elementos usando a função targets::tar_targets() em que o seu primeiro argumento é o nome do target a ser criado e o segundo é a função do script “functions.R” a ser usada, no seguinte esquema:\r\n\r\n\r\nlist(targets::tar_target(nome_do_target_1, funcao_a_ser_executada()), # 1\r\n     targets::tar_target(nome_do_target_2, funcao_a_ser_executada(nome_do_target_1))) # 2\r\n\r\n\r\n\r\nA função usada dentro do targets::tar_targets() geralmente requer como argumento o nome do target anterior, como no caso 2. Entretanto, targets que iniciam o fluxo de trabalho (caso 1) ou que não estão ligadas diretamente aos produtos intermediários da computação não requerem que seja passado algum argumento à sua função.\r\nNeste trabalho, a lista completa de targets definidos de acordo com as funções pode ser observada a seguir:\r\n\r\n\r\nlist(\r\n    targets::tar_target(packages, packages_used()),\r\n    targets::tar_target(dataset, readind_data()),\r\n    targets::tar_target(vol_split, split_data(dataset, 1504, 0.75, cod_volume)),\r\n    targets::tar_target(test_split, test_set_split(vol_split)),\r\n    targets::tar_target(train_split, train_set_split(vol_split)),\r\n    targets::tar_target(vol_resamples, kfold_cv(train_split, strata = cod_volume, repeats = 5)),\r\n    targets::tar_target(normalized_vol_rec, preproc_rec(train_split)),\r\n    targets::tar_target(simple_vol_rec, simple_rec(train_split)),\r\n    targets::tar_target(all_specs, def_specs()),\r\n    targets::tar_target(wflow_vol, workflow_config(all_specs, normalized_vol_rec, simple_vol_rec)),\r\n    targets::tar_target(racing_ctrl, racing_defs()),\r\n    targets::tar_target(results_vol, race_tuning(wflow_vol, vol_resamples, racing_ctrl, 1504)),\r\n    targets::tar_target(plot_rsq, plot_results(results_vol, \"rsq\")),\r\n    targets::tar_target(best_model, select_models(results_vol, \"rmse\", 1)),\r\n    targets::tar_target(intermediary_model, select_models(results_vol, \"rmse\", 10)),\r\n    targets::tar_target(fit_best_mod, fit_model(results_vol, best_model, vol_split)),\r\n    targets::tar_target(fit_intermediary_model, fit_model(results_vol, intermediary_model, vol_split)),\r\n    targets::tar_target(metrics_best, metrics_mod(fit_best_mod, best_model)),\r\n    targets::tar_target(metrics_intermediary, metrics_mod(fit_intermediary_model, intermediary_model)),\r\n    targets::tar_target(model_comparison, accessing_models(fit_best_mod,\r\n                                                           fit_intermediary_model, \r\n                                                           best_model, \r\n                                                           intermediary_model)),\r\n    tarchetypes::tar_render(report, here::here(\"report\", \"report.Rmd\"))\r\n)\r\n\r\n\r\n\r\nEssa lista possui alguns pontos dignos de destaque:\r\nO targets possui algumas ferramentas de avaliação do fluxo de trabalho estabelecido, e uma das mais legais é a função targets::tar_visnetwork. Ela permite que o pipeline seja inspecionado a partir de um fluxograma que informa se os targets estão atualizados ou não.\r\nFigure 4 - Network para inspeção do pipeline criado. Círculo em cor cinza signfica que que o target está desatualizado ou passou por algum ajuste (deve ser computado via targets::tar_make()); círculo laranja indica que há algum erro no pipeline; círculo verde indica que o target está atualizado (up-to-date).Ao fim da lista, o target tarchetypes::tar_render permite que um relatório seja gerado a partir de um arquivo “.Rmd”. Assim, foi produzido um relatório simples, usando rmardown, visando ser uma amostra do que pode ser feito a partir desse orquestramento de trabalho. Esse exemplo pode ser expandido para a produção de um artigo acadêmico ou um relatório mais complexo. Dessa forma, pode-se realizar as análises do trabalho e gerar o material final em um único pipeline. No corpo do relatório produzido, foram adicionados chamados aos 3 penúltimos objetos da lista de targets (metrics_best, metrics_intermediary, model_comparison) usando a função targets::tar_read(), em que estes objetos targets são lidos dentro do relatório. Isso é necessário para que a função tarchetypes::tar_render saiba que o relatório só poderá ser produzido quando todas as análises do pipeline forém finalizadas e estes três últimos objetos gerados.\r\nOutro ponto digno de nota é que esse processamento pode levar um tempo para ser computado. Com isso, rodá-lo novamente pode ser bem tedioso. Uma vantagem do targets é que ele memoriza os passos realizados e guarda os produtos obtidos. Dessa forma, caso seja realizada alguma alteração no pipeline, ele roda apenas as análises diretamente afetadas pela alteração feita. Por exemplo: caso seja adicionado um novo modelo na função def_specs(), que consta no arquivo functions.R, todas as análises diretamente afetadas por esse passo serão computadas novamente, porém os passos anteriores ao seu chamado não são recomputados, reduzindo o tempo e otimizando o trabalho.\r\nReproduzindo o pipeline\r\nApós realização da modelagem e orquestramento do trabalho, resta a sua publicação para que outras pessoas possam reproduzi-lo. Isso é feito de uma forma bem simples graças ao renv e ao targets. Basta clonar o repositório e salvá-lo em uma pasta principal, preferencialmente. Então, basta abrir o arquivo R Project (Eucalipto_Volume.Rproj) no RStudio e executar os seguintes comandos para instalar todos os pacotes necessários e rodar o pipeline, respectivamente:\r\n\r\n\r\nrenv::restore() # Pode demorar um pouco\r\ntargets::tar_make() # Roda o pipeline\r\n\r\n\r\n\r\nCaso você queira inspecionar o pipeline, para acompanhar o processo ou onde possa ter ocorrido possíveis falhas, basta chamar:\r\n\r\n\r\ntargets::tar_visnetwork() # Apresenta o diagrama de execução\r\n\r\n\r\n\r\nConclusão (UFA!)\r\nEssa foi uma introdução (espero que não muito cansativa) à uma das formas de utilização de modelos de machine learning com o tidymodels e de como aumentar a reprodutibilidade de trabalhos com os pacotes here, renv e targets. Confesso que esse projeto foi um trabalho bem bacana de realizar e utilizar estes pacotes se mostrou uma experiência super enriquecedora.\r\nAgradeço quem chegou até o final. Até mais!\r\nReferências\r\nAzevedo, G. B. et al. Multi-volume modeling of Eucalyptus trees using regression and artificial neural networks. Plos one, v. 15, n. 9, p. e0238703, 2020.\r\nKuhn, M., Silge, J., 2021. Tidy Modeling with R. URL: https://www.tmwr.org/\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-20-predizendo-volume-de-eucalipto-com-tidymodels-xgboost-e-targets/featured.jpg",
    "last_modified": "2021-10-20T01:10:52-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-20-xgboost-com-python-para-biometria-florestal/",
    "title": "XGBoost com Python para biometria florestal",
    "description": "Como ajustar um modelo XGBoost para predição do volume de eucalipto.",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2021-10-20",
    "categories": [],
    "contents": "\r\nNo último texto que escrevi, o XGBoost foi o melhor modelo ajustado dentre todos os que foram testados. Um colega que leu o texto me solicitou um auxílio para ajustar este modelo usando Python, já que essa é a linguagem que ele possui maior domínio. Então decidi aproveitar e escrever um breve tutorial sobre esse ajuste.\r\nO dataset aqui utilizado é oriundo do excelente trabalho publicado publicado por Azevedo et al. (2020).\r\nAprendendo com os erros\r\nXGBoost (eXtreme Gradient Boosting) é uma implementação eficiente do algoritmo gradient boosted trees. Este algoritmo é um dos mais usados atualmente dada a sua elevada capacidade preditiva, além de ter como ponto forte a sua escalabilidade em diversos cenários.\r\nDe forma geral, os algoritmos gradient boosting atuam de modo a predizer uma variável resposta a partir da combinação de estimativas de um set de modelos fracos.\r\nO processo de treinamento desses modelos se dá de forma iterativa, em que são adicionada novas árvores de decisão que tem o objetivo de predizer os resíduos das árvores estabelecidas previamente, “aprendendo” com erros das árvores ajustadas anteriormente. São chamados de gradient boosting models por usarem o algoritmo de gradiente descendente para minimizar a função de perda. Mais informações sobre esses algoritmos podem ser encontradas aqui, aqui, aqui e aqui.\r\nAjustando o modelo\r\nImportando as bibliotecas necessárias:\r\n\r\nfrom xgboost import XGBRegressor\r\nfrom sklearn import metrics\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nNeste caso, o dataset já estava particionado. Porém, optei por realizar a sua unificação de modo a seguir o padrão estipulado no texto anterior.\r\n\r\ntraining = pd.read_excel('data/train.xlsx', skiprows=1)\r\ntesting = pd.read_excel('data/test.xlsx', skiprows=1)\r\n\r\ndf = training.append(testing)\r\n\r\nX = df.filter(items=[\"DBH\", \"H\", \"TX\", \"d\"]).values.reshape(-1, 4)\r\ny = df.filter(items=[\"V\"])\r\n\r\nParticionando os dados em treino e teste com estratificação de acordo com o volume comercial:\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, \r\n                                                    test_size=0.25, \r\n                                                    random_state=1504,\r\n                                                    stratify=df.Cod_Volume)\r\n\r\nEm seguida, defini o modelo e seus hiperparâmetros. Optei por não realizar a tunagem destes hiperparâmetros e usar os valores já definidos no último tutorial:\r\n\r\nmodel = XGBRegressor(\r\n    objective='reg:squarederror',\r\n    n_estimators=2000,\r\n    max_depth=7,\r\n    learning_rate=0.0367,\r\n    n_jobs=10,\r\n    gamma=0.0000806,\r\n    booster=\"gbtree\",\r\n    min_child_weight=20\r\n    )\r\n\r\nFeita esta estapa, resta ajustar o modelo com os dados de treino:\r\n\r\nmodel.fit(X_train, y_train)\r\n\r\nRealizado o ajuste, agora será realizada a predição dos dados de teste para comparação e avaliação da qualidade do modelo:\r\n\r\npredicted_vol = model.predict(X_test)\r\n\r\n# Avaliação da qualidade do modelo usando o R²\r\nprint(\"Coeficiente de determinação - R²:\", metrics.r2_score(y_test, predicted_vol))\r\n\r\nO modelo apresentou um R² bastante elevado, indicando potencial de uso para a predição do volume de eucalipto.\r\nObtendo uma apresentação visual dos resultados do volume predito com o modelo XGBoost e o volume observado em campo:\r\n\r\nmodel_results = pd.DataFrame({\"actual\": y_test[\"V\"].array, \r\n                              \"predicted\": predicted_vol}\r\n)\r\n\r\nplt.figure(figsize=(8, 7), dpi=300)\r\nplt.scatter(model_results[\"actual\"], \r\n            model_results[\"predicted\"],\r\n            s=15, \r\n            edgecolors='black', \r\n            linewidth=0.4, \r\n            alpha=0.6)\r\nplt.title(\"Volumetria Eucalipto - Modelo XGBoost\")\r\nplt.xlabel(\"Volume observado (m³)\")\r\nplt.ylabel(\"Volume predito (m³)\")\r\nz = np.polyfit(model_results[\"actual\"], \r\n               model_results[\"predicted\"], \r\n               1)\r\np = np.poly1d(z)\r\nplt.plot(model_results[\"predicted\"], \r\n         p(model_results[\"predicted\"]), \r\n         \"r--\", \r\n         color=\"black\")\r\n\r\nVolume (m³) de eucalipto observado vs predito.Como podemo concluir que o modelo generaliza bem, basta ajustá-lo usando toda a base de dados:\r\n\r\nmodel.fit(X, y)\r\n\r\nPronto! Agora basta escolher a melhor forma de salvar seu modelo seja usando pickle ou joblib (papo para outro texto).\r\nRepositório: https://github.com/TheilonMacedo/volumetria_xgboost.\r\nReferências\r\nAzevedo, G. B. et al. Multi-volume modeling of Eucalyptus trees using regression and artificial neural networks. Plos one, v. 15, n. 9, p. e0238703, 2020.\r\nhttps://estatsite.com.br/2020/10/03/xgboost-em-python/\r\nJames, G. et al. An introduction to statistical learning. New York: springer, 2013.\r\nhttps://xgboost.readthedocs.io/en/latest/\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-20-xgboost-com-python-para-biometria-florestal/scatter_vol.png",
    "last_modified": "2021-11-22T22:48:01-03:00",
    "input_file": {}
  }
]
