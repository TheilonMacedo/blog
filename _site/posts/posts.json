[
  {
    "path": "posts/2022-09-09-como-filtrar-colunas-de-forma-eficiente-no-pandas/",
    "title": "Como filtrar colunas de forma eficiente no Pandas",
    "description": "Como realizar a filtragem de colunas de forma mais legível e rápida em Python.",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2022-09-09",
    "categories": [],
    "contents": "\r\nCriando um dataset\r\nEsse é um post rápido sobre uma feature da biblioteca\r\npandas que aprendi recentemente ao manipular dados\r\ntabulares em Python. Primeiro, vamos criar um dataset sintético para\r\npodermos usar as features da biblioteca:\r\n\r\nfrom random import uniform\r\n\r\nimport pandas as pd\r\n\r\n\r\nforest_info = pd.DataFrame(\r\n    {\r\n        \"trees\": [\"Eucalyptus urophylla\", \"Paubrasilia echinata\"] * 10, # 1\r\n        \"height\": [uniform(10.0, 20.0) for _ in range(20)], # 2\r\n        \"dbh\": [uniform(5.0, 25.0) for _ in range(20)], # 3\r\n        \"plots\": range(1, 21), # 4\r\n    }\r\n)\r\n\r\nO dataset no pandas pode ser criado a partir de dicionários. Os\r\npassos usados foram os seguintes:\r\n1 - Criar a coluna trees a partir de uma list com dois\r\nitens (nomes de árvores) e repetí-los 10 vezes (20 no total);\r\n2 - Criar a coluna height a partir de uma list\r\ncomprehension com 20 valores float entre 10.0 e 20.0 representando a\r\naltura das árvores;\r\n3 - Criar a coluna dbh (Diameter Breast Height) a partir de\r\numa list comprehension com 20 valores float entre 5.0 e 25.0\r\nrepresentando o diametro das árvores;\r\n4 - Criar a coluna plots a partir de um generator usando a função\r\nrange de 1 a 21 (20 valores) representando as parcelas onde\r\nas árvores se encontram;\r\nPronto! Temos nosso dataset e agora vamos colocar a mão na massa.\r\nFiltrando o dataset\r\nImagine que devemos filtrar as árvores de\r\nEucalyptus urophylla e com altura maior que 15 metros.\r\nGeralmente a filtragem de colunas por valores no pandas é\r\nrealizada usando a seguinte sintaxe:\r\n\r\nforest_info[forest_info[\"trees\"] == \"Eucalyptus urophylla\"]\r\n\r\n# Output\r\n#                    trees     height        dbh  plots\r\n# 0   Eucalyptus urophylla  16.738678   8.672227      1\r\n# 2   Eucalyptus urophylla  10.842217  11.713358      3\r\n# 4   Eucalyptus urophylla  15.219263  13.234271      5\r\n# 6   Eucalyptus urophylla  12.622703  15.547701      7\r\n# 8   Eucalyptus urophylla  11.283926  15.129853      9\r\n#                         ...\r\n\r\nforest_info[forest_info[\"height\"] > 15]\r\n\r\n#                    trees     height        dbh  plots\r\n# 0   Eucalyptus urophylla  16.738678   8.672227      1\r\n# 1   Paubrasilia echinata  18.450128  15.971558      2\r\n# 3   Paubrasilia echinata  17.418030  15.862028      4\r\n# 4   Eucalyptus urophylla  15.219263  13.234271      5\r\n# 12  Eucalyptus urophylla  17.113081  11.347302     13\r\n#                        ...\r\n\r\nParticularmente não curto muito essa sintaxe, pois acho ela um pouco\r\n“congestionada” de informações, principalmente quando se está\r\ntrabalhando com datasets muito grandes. Caso queira-se aplicar dois\r\nfiltros em uma mesma operação, fica um pouco mais complexo:\r\n\r\nforest_info[(forest_info[\"trees\"] == \"Eucalyptus urophylla\") & (forest_info[\"height\"] > 15)]\r\n\r\n#                    trees     height        dbh  plots\r\n# 0   Eucalyptus urophylla  18.415341  20.949509      1\r\n# 2   Eucalyptus urophylla  19.877132   9.607769      3\r\n# 8   Eucalyptus urophylla  15.901298  19.122095      9\r\n# 10  Eucalyptus urophylla  19.013848   8.693424     11\r\n#                        ...\r\n\r\nUma forma que encontrei que torna essa filtragem mais simples é\r\nutilizar o método .query() do pandas:\r\n\r\n# Filtrando as espécies de árvores\r\nforest_info.query(\"trees == 'Eucalyptus urophylla'\")\r\n\r\n#                   trees     height        dbh  plots\r\n# 0   Eucalyptus urophylla  18.415341  20.949509      1\r\n# 2   Eucalyptus urophylla  19.877132   9.607769      3\r\n# 4   Eucalyptus urophylla  12.391588  13.922425      5\r\n# 6   Eucalyptus urophylla  11.594747  17.113775      7\r\n#                        ...\r\n\r\n# Filtrando a altura das árvores\r\nforest_info.query(\"height > 15\")\r\n\r\n#                    trees     height        dbh  plots\r\n# 0   Eucalyptus urophylla  18.415341  20.949509      1\r\n# 2   Eucalyptus urophylla  19.877132   9.607769      3\r\n# 8   Eucalyptus urophylla  15.901298  19.122095      9\r\n# 10  Eucalyptus urophylla  19.013848   8.693424     11\r\n#                        ...\r\n\r\n# Unificando os filtros\r\nforest_info.query(\"trees == 'Eucalyptus urophylla' and height > 15\")\r\n#                    trees     height        dbh  plots\r\n# 0   Eucalyptus urophylla  18.415341  20.949509      1\r\n# 2   Eucalyptus urophylla  19.877132   9.607769      3\r\n# 8   Eucalyptus urophylla  15.901298  19.122095      9\r\n# 10  Eucalyptus urophylla  19.013848   8.693424     11\r\n#                        ...\r\n\r\nEsse método funciona da seguinte forma:\r\n{dataframe}.query(nome_coluna {operador}\r\nvalores). Acredito que essa sintaxe seja bem mais amigável\r\ne de fácil leitura, principalmente pra quem vem do R ou do SQL, sendo\r\nmais clean que a sintaxe mais comum.\r\nBônus\r\nAlgo que quebrei a cabeça para realizar essa semana foi a filtragem\r\nde uma coluna usando uma lista de valores. Descobri que no pandas pode\r\nser feito a partir do método .isin(), desse modo:\r\n\r\n# List com espécies que quero filtrar\r\nspecies = [\"Eucalyptus urophylla\", \"Eucalyptus grandis\"]\r\n\r\n# Filtrando a coluna de espécies do dataframe com a lista de espécies de interesse\r\nforest_info[forest_info.trees.isin(species)]\r\n\r\n#                    trees     height        dbh  plots\r\n# 0   Eucalyptus urophylla  18.415341  20.949509      1\r\n# 2   Eucalyptus urophylla  19.877132   9.607769      3\r\n# 4   Eucalyptus urophylla  12.391588  13.922425      5\r\n# 6   Eucalyptus urophylla  11.594747  17.113775      7\r\n#                        ...\r\n\r\nSimilar aos exemplos anteriores, achei a sintaxe um pouco poluída.\r\nPara contornar isso, mais uma vez o método query ao\r\nresgate:\r\n\r\n# Pode ser usada qualquer das duas formas \r\nforest_info.query(f\"trees in {species}\")\r\nforest_info.query(\"trees in @species\")\r\n\r\n#                    trees     height        dbh  plots\r\n# 0   Eucalyptus urophylla  18.415341  20.949509      1\r\n# 2   Eucalyptus urophylla  19.877132   9.607769      3\r\n# 4   Eucalyptus urophylla  12.391588  13.922425      5\r\n# 6   Eucalyptus urophylla  11.594747  17.113775      7\r\n#                        ...\r\n\r\nNesse caso pode-se usar tanto uma f-string como\r\nadicionar o operator @ à frente do objeto criado\r\n(species).\r\nCom essa mesma sintaxe, pode-se realizar a filtragem de colunas usando\r\noutras colunas:\r\n\r\nfrom random import randint\r\n\r\n# Criando um dataframe com plots (5) de interesse\r\nexp = pd.DataFrame({\"treat\": [randint(1, 6) for _ in range(5)]})\r\n\r\n# Filtrando a coluna de plots do `forest_info` com a coluna 'treat' do 'exp' usando `isin`\r\nforest_info[forest_info.plots.isin(exp.treat)]\r\n\r\n#                   trees     height        dbh  plots\r\n# 1  Paubrasilia echinata  12.920267  10.672437      2\r\n# 2  Eucalyptus urophylla  19.877132   9.607769      3\r\n# 3  Paubrasilia echinata  10.525335  12.740519      4\r\n# 4  Eucalyptus urophylla  12.391588  13.922425      5\r\n# 5  Paubrasilia echinata  14.135821   6.524656      6\r\n\r\n# Filtrando a coluna de plots do `forest_info` com a coluna 'treat' do 'exp' usando `isin` \r\n# dentro do método `query` e obtendo os valores da coluna\r\n\r\nforest_info.query(\"plots.isin(@exp.treat).values\")\r\n\r\n#                   trees     height        dbh  plots\r\n# 1  Paubrasilia echinata  12.920267  10.672437      2\r\n# 2  Eucalyptus urophylla  19.877132   9.607769      3\r\n# 3  Paubrasilia echinata  10.525335  12.740519      4\r\n# 4  Eucalyptus urophylla  12.391588  13.922425      5\r\n# 5  Paubrasilia echinata  14.135821   6.524656      6\r\n\r\nPor hoje é isso. Até a próxima!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-09-09T21:05:29-03:00",
    "input_file": "como-filtrar-colunas-de-forma-eficiente-no-pandas.knit.md"
  },
  {
    "path": "posts/2022-07-23-web-scraping-com-python-e-dataviz-com-ggplot2/",
    "title": "Web scraping com Python e dataviz com ggplot2",
    "description": "Como obter dados do site do IMDb e criar gráficos com ggplot2 no R.",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2022-07-23",
    "categories": [],
    "contents": "\r\nEsse tutorial é bem rápido e explica como realizar a obtenção de\r\ndados sobre a série de TV “The Boys” usando Python e realizar a criação\r\nde um gráfico bonito usando o pacote ggplot2 do R.\r\n1 - Tarefa inicial:\r\nweb scraping no site do IMDb\r\nA primeira tarefa é a obtenção da base de dados do site do IMDb.\r\nNesse caso, será feita a aquisição das informações sobre a nota dos\r\nepisódios de modo automático, usando a lib selenium.\r\nPrimeiro importe todas as bobliotecas e os métodos a serem\r\nusados:\r\n\r\nimport os\r\nimport time\r\n\r\nimport pandas as pd\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.chrome.options import Options\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.webdriver.support.ui import Select, WebDriverWait\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\n\r\nEm seguida, defina a endereço da página e o Xpath para o botão de\r\nmusança de temporada como constates que serão usadas ao longo do\r\ntutorial:\r\n\r\nPAGE_URL = \"https://www.imdb.com/title/tt1190634/\" \r\nEPISODES_BUTTON = \"/html/body/div[2]/main/div/section[1]/section/div[3]/section/section/div[1]/div/div[1]/a/span[1]\"\r\n\r\nAgora serão defindas as opções de operação do navegador, visando que\r\na janela do Chrome abra sem prolemas:\r\n\r\noptions = Options()\r\noptions.binary_location = os.environ.get(\"GOOGLE_CHROME_BIN\")\r\noptions.add_argument(\"--no-sandbox\")\r\noptions.add_argument(\"--disable-dev-shm-usage\")\r\noptions.add_argument(\"--disable-gpu\")\r\noptions.add_argument(\"start-maximized\")\r\n\r\n2 - Chegou a hora da diversão\r\nAgora iremos solicitar ao navegador a sua abertura e que ele vá até o\r\nendereço definido previamente. Em seguida, é realizado o click no botão\r\nque define a seleção de temporadas. Por fim, então é definida um seletor\r\nde opções e a lista de temporadas da referida série de TV. A ideia é que\r\nsejam obtidos dados de cada temporada a partir de um loop baseado no\r\ntamanho da lista obtida e em seguida, para cada temporada, seja clicado\r\no seu botão correspondente:\r\n\r\ndriver = webdriver.Chrome(\r\n    service=Service(ChromeDriverManager().install()), options=options\r\n)\r\n\r\ndriver.get(PAGE_URL)\r\n\r\nWebDriverWait(driver, 10).until(\r\n    EC.element_to_be_clickable((By.XPATH, EPISODES_BUTTON))\r\n).click()\r\n\r\nselector_options = driver.find_element(By.XPATH, '//*[@id=\"bySeason\"]')\r\nseasons = len([x for x in selector_options.find_elements(By.TAG_NAME, \"option\")])\r\n\r\nAgora vem a parte legal da coisa. Primeiros inicializamos uma lista\r\ncom as variáveis usadas para a armazenamento das variáveis dos dados\r\nobtidos.\r\n\r\nratings = list()\r\nairdates = list()\r\ntitles = list()\r\nnumber = list()\r\n\r\nEm seguida é definido um for loop meio grandinho, mas\r\ncalma que explico:\r\n\r\nfor i in range(seasons - 1):\r\n    select = Select(driver.find_element(By.XPATH, '//*[@id=\"bySeason\"]')) # 1\r\n    select.select_by_index(i)\r\n    time.sleep(10)\r\n\r\n    # Ratings\r\n    episodes = driver.find_elements(By.CLASS_NAME, \"ipl-rating-star__rating\") # 2\r\n    for i in episodes:\r\n        try:\r\n            ratings.append(float(i.text))\r\n        except ValueError:\r\n            pass\r\n\r\n    # Airdates\r\n    dates_1 = driver.find_elements(By.CLASS_NAME, \"airdate\") # 3\r\n    dates_2 = [i.text for i in dates_1]\r\n    airdates.extend(dates_2)\r\n\r\n    # Titles\r\n    ep_names_1 = driver.find_elements(By.XPATH, \"//a[@title]\") # 4\r\n    ep_names_2 = [i.text for i in ep_names_1 if i.text != \"\"][1::2]\r\n    titles.extend(ep_names_2)\r\n\r\n    # Episode Number\r\n    ep_number_1 = driver.find_elements(By.XPATH, \"//a[@title]\")\r\n    ep_number_2 = [i.text for i in ep_names_1 if i.text != \"\"][0::2]\r\n    number.extend(ep_number_2)\r\n\r\n1 - A primeira coisa é definir um seletor pra… selecionar a opção do\r\nmenu dropdown com a lista de temporadas. Após a seleção, o sistema\r\naguarda por 10 segundos (tempo da página carregar).\r\n2 - É obtida a lista de elementos contendo as avaliações dos\r\nepisódios, então é definido um for loop (mais um) em que é\r\napensada à lista ratings cada episódio, sendo cada item a\r\nlista episodes convertida para o tipo float.\r\nCaso a conversão dê errado, nada acontece, feijoada (sim, o meme é\r\nvelho).\r\n3 - Mesmo processo: é obtida uma lista de elementos contendo as infos\r\ndas datas em que os eposódios foram ao ar, obtido o texto desses\r\nelementos, que então são adicionados à lista airdates.\r\n4 - A obtenção dos títulos e do número dos epísódios é feita de forma\r\nsimilar ao passo #3. A diferença aqui é que é realizado um filtro para\r\ncampos em branco e a seleção em formato de lista pela notação\r\n[start:step:end].\r\n3 - Salvando os dados\r\nEntão é criado um dataframe dos pandas com as listas obtidas e em\r\nseguida ele é salvo em formato .csv.\r\n\r\npd.DataFrame(\r\n    {\"number\": number, \"ratings\": ratings, \"airdates\": airdates, \"titles\": titles}\r\n).to_csv(\"imdb_data.csv\", index=False)\r\n\r\nA segunda parte consiste na criação do gráfico usando o pacote\r\nggplot2.\r\n4 - Criação do gráfico\r\nOs pacotes do R usados para esse gráfico são:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(ggdark)\r\nlibrary(cowplot)\r\nlibrary(magick)\r\nlibrary(colorspace)\r\nlibrary(ggfx)\r\nlibrary(ggrepel)\r\nlibrary(gridExtra)\r\nlibrary(extrafont)\r\n\r\n\r\n\r\nEntão são carregadas as fontes do pacote extrafont e definido o\r\nformato de datas:\r\n\r\n\r\nloadfonts()\r\n\r\nSys.setlocale(\"LC_TIME\", \"English\")\r\n\r\n\r\n\r\n5 - A primeira etapa cosniste na definição da paleta de cores,\r\nleitura da base e limpeza dos dados:\r\n\r\n\r\ntb_cols <- c(\"#de0909\", \"#fcfafa\", \"#e6d309\", \"#999999\")\r\nraw_imdb_data <- readr::read_csv(\"D:/Projetos/the_boys/imdb_data.csv\")\r\n\r\nraw_imdb_data %>% \r\n    mutate(airdates = gsub(\"\\\\.\", \"\", airdates)) %>% \r\n    mutate(airdates = as.Date(airdates, format = \"%d %b %Y\")) %>% \r\n    mutate(episode = as.integer(substring(number, 7))) %>% \r\n    mutate(season = as_factor(substring(number, 1, 2))) %>% \r\n    mutate(episode_all = 1:24) %>% \r\n    select(-number) -> imdb_data\r\n\r\n\r\n\r\n6 - Com os dados bonitinhos, vamos criar o gráfico e apresentar os\r\nsteps necessários:\r\n\r\n\r\nggplot(imdb_data, aes(episode_all, ratings)) + # definição do número de episódios em x e avaliações em y\r\n    with_blur(geom_smooth(aes(col = season), # adicção de linhas de tendência mais softs\r\n                          formula = y ~ x,\r\n                          method = \"lm\",\r\n                          se = F,\r\n                          size = 0.6)) +\r\n    with_blur(geom_point( # Adição dos pontos que correspondem aos episódios soft\r\n        aes(color = season),\r\n        size = 2.5)\r\n    ) +\r\n    geom_point(aes(color = season), # Adição dos pontos que correspondem aos episódios \"bruta\"\r\n        size = 1.5) +\r\n    scale_color_manual(values = c(\"#de0909\", \"#fcfafa\", \"#e6d309\")) + # definição das cores\r\n    scale_y_continuous( # definição do eixo y\r\n        name = \"IMDb Users Ratings\",\r\n        limits = c(7.3, 10),\r\n        breaks = c(7.5, 8.0, 8.5, 9.0, 9.5, 10.0)\r\n    ) +\r\n    xlab(\"Episode Number\") + # rótulo do eixo x\r\n    labs(title = \"IMDB Ratings of all The Boys Episodes\", # definição do título e da legenda\r\n         subtitle = \"\r\n    The Boys is a superhero television series developed by Eric Kripke for \r\n    Amazon Prime Video. Based on the comic book of the same name by Garth \r\n    Ennis and Darick Robertson, it follows the eponymous team of vigilantes\r\n    as they combat superpowered individuals who abuse their superpowers.\",\r\n    color = \"Season\") +\r\n    dark_theme_classic() + # adicionando um tema preto\r\n    theme(panel.grid.major.y = element_line(color = \"gray50\"), # definição geral do gráfico (fonte, legenda...)\r\n          axis.line.y = element_blank(),\r\n          text=element_text(size=14, family = \"Noto Sans\"),\r\n          plot.margin = margin(0.5, 7, 0.5, 0.5, \"cm\"),\r\n          plot.subtitle=element_text(size=10, face=\"italic\", hjust=-0.03),\r\n          axis.text = element_text(color = \"white\"),\r\n          legend.position = \"bottom\") +\r\n    with_blur(annotate(\r\n        geom = \"curve\", x = 19, y = 9.9, xend = 21.8, yend = 9.73, # adição da seta superior\r\n        curvature = -0.3, arrow = arrow(length = unit(2, \"mm\"))) \r\n    ) +\r\n    annotate(geom = \"text\",x = 18.85, y = 9.9, label = \"Herogasm\", # adição da legenda da seta superior\r\n             hjust = \"right\", family = \"Noto Sans\") +\r\n    with_blur(annotate( # adição da seta inferior \r\n        geom = \"curve\", x = 8.0, y = 7.55, xend = 9.85, yend = 7.65, \r\n        curvature = 0.3, arrow = arrow(length = unit(2, \"mm\")))\r\n    ) +\r\n    annotate(geom = \"text\", x = 7.85, y = 7.55, # adição da legenda da seta inferior\r\n             label = \"Proper Preparation and Planning\", hjust = \"right\", \r\n             family = \"Noto Sans\") -> my_plot # salvando o gráfico\r\n\r\n\r\n\r\n7 - customizando o gráfico (mais ainda)\r\nAdicionando a logo da série ao gráfico:\r\n\r\n\r\nlogo <- image_read(here::here(\"logo.png\"))\r\nggdraw() +\r\n    draw_plot(my_plot) +\r\n    draw_image(logo, x = 0.35, y = 0.4, scale = .2) -> plot1\r\n\r\n\r\n\r\n8 - Criando o gráfico de “quadrinhos” heatmap:\r\n\r\n\r\nimdb_data %>% \r\n    ggplot(aes(season, episode, label = ratings)) + # temporadas no x e episódios no y\r\n    with_blur(geom_tile(aes(fill = ratings), # adicionando o heatmap\r\n              color = \"white\",\r\n              lwd = 0.4,\r\n              linetype = 1,\r\n              width=0.94,\r\n              height=0.94)) +\r\n    scale_fill_gradient(low = tb_cols[3], high = tb_cols[1]) + # definindo as cores\r\n    scale_y_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8), # customizando o eixo y\r\n                       trans = \"reverse\") +\r\n    scale_x_discrete(position = \"top\") + # colocando o eixo x no topo\r\n    xlab(\"Season\") + # rótulo do eixo x\r\n    ylab(\"Episodes\") + # rótulo do eixo y\r\n    guides(fill = guide_colourbar(barwidth = 0.35, # definindo a espessura da grade\r\n                                  barheight = 5)) +\r\n    theme(axis.line = element_blank(), # definição geral do gráfico (fonte, legenda...)\r\n          axis.ticks = element_blank(),\r\n          text = element_text(family = \"Noto Sans\"),\r\n          aspect.ratio=10/3,\r\n          legend.title=element_blank(),\r\n          legend.text=element_text(size=7, colour = \"white\"),\r\n          panel.background = element_rect(fill='transparent'),\r\n          plot.background = element_rect(fill='transparent', color=NA),\r\n          panel.grid.major = element_blank(), #remove major gridlines\r\n          panel.grid.minor = element_blank(), #remove minor gridlines\r\n          legend.background = element_rect(fill='transparent'), #transparent legend bg\r\n          plot.margin = margin(0.0, 0, 0.0, 0.0, \"cm\"),\r\n          axis.text = element_text(color = \"white\"),\r\n          axis.title = element_blank()) -> plot2 # # salvando o gráfico\r\n\r\n\r\n\r\n9 - Unindo os dois gráficos\r\nApós preparar os dois gráficos, então é feita a sua união e definição\r\nda localização de cada um no plot:\r\n\r\n\r\nggdraw() +\r\n    draw_plot(plot1, 0, 0, 1, 1) +\r\n    draw_plot(plot2, 0.79, 0.19, 0.2, 0.55)\r\n\r\n\r\n\r\nPS: a definição do tamanho de salvamento pode afetar a apresentação\r\ndos elementos nos gráficos.\r\nÁreas do Parque Nacional do Alto Cariri,\r\nna Bahia.Conlusão\r\nOpa, mais um tutorial para a lista! Espero que tenha gostado e até a\r\npróxima!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-23-web-scraping-com-python-e-dataviz-com-ggplot2/Rplot.png",
    "last_modified": "2022-07-23T16:49:41-03:00",
    "input_file": "web-scraping-com-python-e-dataviz-com-ggplot2.knit.md"
  },
  {
    "path": "posts/2022-06-08-automao-de-raspagem-de-dados/",
    "title": "Automação de raspagem de dados e envio de notificação via Telegram",
    "description": "Como automatizar a aquisição de dados e envio de notificações com Selenium e Heroku.",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2022-06-08",
    "categories": [],
    "contents": "\r\nEste tutorial descreve as etapas necessárias para a aquisição de\r\ndados, geração de gráficos e envio de notificação via Telegram de forma\r\nautomatizada usando Python e Heroku.\r\nEtapa 1\r\nA pasta de trabalho foi organizada usando a seguinte estrutura:\r\n.\r\n├── Procfile\r\n├── README.rst\r\n├── pyproject.toml\r\n├── requirements.txt\r\n├── runtime.txt\r\n├── setup.sh\r\n├── src\r\n│   └── de_project\r\n│       ├── __init__.py\r\n│       ├── get_table_pmi.py\r\n│       ├── load_data.py\r\n│       ├── main.py\r\n│       ├── plots_monitor.py\r\n│       ├── send_msg.py\r\n│       └── usd_brl.py\r\n└── tests\r\n    ├── __init__.py\r\n    └── test_de_project.py  \r\n\r\nMais à frente descrevo as funções de cada arquivo nesta pasta. Por\r\nenquanto, explicarei o código dos arquivos get_table_pmi.py\r\ne e usd_brl.py. A ideia de dividir o código em módulos foi\r\npara facilitar a sua organização e manutenção. Começando pelo módulo\r\nget_table_pmi.py, a ideia aqui é a obtenção de uma base de\r\ndados relativa ao Índice de atividade dos gerentes de compras (PMI)\r\nServiços Caixin que funciona como um termômetro da economia chinesa a\r\nnível mensal. Para isso foi utilizada a biblioteca Selenium de modo a\r\nacessar a página e obter a raspar a base de dados. A ideia é que este\r\ncódigo rode no background de um servidor virtual em um serviço na nuvem\r\n(Heroku). Alguns Key Insights dessa etapa são:\r\nO tratamento da base de dados necessária após a sua aquisição na url\r\ndefinida;\r\nO salvamento dessa base utilizando a lib sqlalchemy de modo a termos\r\numa database de tamanho diminuto e acesso rápido;\r\nA utilização de Logging para informativos acerca da execução do\r\nprograma.\r\n\r\n# get_table_pmi.py\r\nfrom time import sleep\r\n\r\n\r\ndef get_data_pmi():\r\n    import logging\r\n    import os\r\n    import re\r\n    import time\r\n\r\n    import arrow\r\n    import numpy as np\r\n    import pandas as pd\r\n    from selenium import webdriver\r\n    from selenium.webdriver.chrome.options import Options\r\n    from selenium.webdriver.chrome.service import Service\r\n    from selenium.webdriver.common.by import By\r\n    from selenium.webdriver.support import expected_conditions as EC\r\n    from selenium.webdriver.support.ui import WebDriverWait\r\n    from sqlalchemy import create_engine\r\n    from webdriver_manager.chrome import ChromeDriverManager\r\n\r\n    logging.basicConfig(format=\"%(levelname)s - %(message)s\", level=logging.DEBUG)\r\n\r\n    logging.info(\"Starting to perform data collection...\")\r\n\r\n    INVESTING_PAGE_URL = (\r\n        \"https://www.investing.com/economic-calendar/chinese-caixin-services-pmi-596\"\r\n    )\r\n\r\n    options = Options()\r\n    options.binary_location = os.environ.get(\"GOOGLE_CHROME_BIN\")\r\n    options.add_argument(\"--headless\")\r\n    options.add_argument(\"--no-sandbox\")\r\n    options.add_argument(\"--disable-dev-shm-usage\")\r\n    options.add_argument(\"window-size=1400x800\")\r\n    options.add_argument(\"--disable-gpu\")\r\n    options.add_argument(\"--remote-debugging-port=9222\")\r\n    options.add_argument(\"start-maximized\")\r\n    print(\"Teste\")\r\n    driver = webdriver.Chrome(\r\n        service=Service(ChromeDriverManager().install()), options=options\r\n    )\r\n    driver.get(INVESTING_PAGE_URL)\r\n    time.sleep(10)\r\n\r\n    logging.info(\"Getting PMI values...\")\r\n\r\n    for i in range(20):\r\n        WebDriverWait(driver, 10).until(\r\n            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"showMoreHistory596\"]/a'))\r\n        ).click()\r\n        time.sleep(2)\r\n\r\n    body = driver.find_elements(By.CSS_SELECTOR, \"#eventHistoryTable596 > tbody\")\r\n    for i in body:\r\n        get = pd.Series(i.text)\r\n\r\n    driver.close()\r\n    driver.quit()\r\n\r\n    logging.info(\"Starting data storing process...\")\r\n\r\n    def split_in_date(text):\r\n        return re.split(r\"((?:0?[1-9][0-2]):[0-5][0-9])\", text)\r\n\r\n    values_list = [split_in_date(i) for i in list(get[0].split(\"\\n\"))]\r\n\r\n    month_list = [\r\n        \"(Jan)\",\r\n        \"(Feb)\",\r\n        \"(Mar)\",\r\n        \"(Apr)\",\r\n        \"(May)\",\r\n        \"(Jun)\",\r\n        \"(Jul)\",\r\n        \"(Aug)\",\r\n        \"(Sep)\",\r\n        \"(Oct)\",\r\n        \"(Nov)\",\r\n        \"(Dec)\",\r\n    ]\r\n\r\n    month_replacement = {i: \"\" for i in month_list}\r\n\r\n    def replace_all(text, dic):\r\n        for i, j in dic.items():\r\n            text = text.replace(i, j)\r\n            string_1 = text.strip()\r\n            string_2 = arrow.get(string_1, \"MMM DD, YYYY\").format(\"YYYY-MM-DD\")\r\n        return string_2\r\n\r\n    date = [replace_all(i[0], month_replacement) for i in values_list]\r\n    close = [i[1] for i in values_list]\r\n    time_data = pd.DataFrame({\"date\": date, \"close\": close})\r\n\r\n    logging.info(\"Time data acquired...\")\r\n\r\n    def get_pmi_values(record_list, col_names):\r\n        pmi_values = [\r\n            i[2].replace(\"   \", \" NaN \").strip().split(\" \") for i in record_list\r\n        ]\r\n        df = pd.DataFrame()\r\n        for i, k in enumerate(col_names):\r\n            vals = [j[i] for j in pmi_values]\r\n            df[k] = vals\r\n        return df\r\n\r\n    numeric_cols = [\"actual\", \"forecast\", \"previous\"]\r\n\r\n    price_data = get_pmi_values(values_list, numeric_cols)\r\n\r\n    logging.info(\"PMI values acquired. Storing data...\")\r\n\r\n    final_data = pd.concat([time_data, price_data], axis=1)\r\n\r\n    final_data[numeric_cols] = (\r\n        final_data[numeric_cols].replace(\"NaN\", np.NaN).apply(pd.to_numeric)\r\n    )\r\n\r\n    disk_engine = create_engine(\"sqlite:///pmi.db\")\r\n\r\n    def write_to_disk(df):\r\n        df.to_sql(\"pmi\", disk_engine, if_exists=\"append\", index=False)\r\n\r\n    write_to_disk(final_data)\r\n\r\n    logging.info(\"Data stored successfully.\")\r\n    sleep(10)\r\n\r\nTambém foi obtida uma base de dados da cotação mensal histórica do\r\ndólra frente ao real. O código segue a mesma linha da base obtida\r\nanteriormente, porém com algumas peculiaridades:\r\nNessa base de dados foi necessário acesso à page do site\r\ninvesting.com usando login e senha (armazenadas como variáveis de\r\nambiente);\r\nFoi necessário também a inserção de campos de filtragem da base de\r\ndados a ser apresentada, como a data do período analisado;\r\nFoi necessário um tratamento de dados de forma mais complexa,\r\nprincipalmente pelo fato da coluna de Data possuir um\r\nformato peculiar (MMM AAA).\r\nPerceba que ambas as bases foram estruturadas dentro de funções.\r\n\r\n#usd_brl.py\r\n\r\ndef get_data_usd_brl():\r\n    import logging\r\n    import os\r\n    import time\r\n    from datetime import date\r\n    from typing import List\r\n\r\n    import pandas as pd\r\n    import unidecode\r\n    from dotenv import load_dotenv\r\n    from selenium import webdriver\r\n    from selenium.webdriver.chrome.options import Options\r\n    from selenium.webdriver.chrome.service import Service\r\n    from selenium.webdriver.common.by import By\r\n    from selenium.webdriver.support import expected_conditions as EC\r\n    from selenium.webdriver.support.ui import Select, WebDriverWait\r\n    from sqlalchemy import create_engine\r\n    from webdriver_manager.chrome import ChromeDriverManager\r\n\r\n    logging.basicConfig(format=\"%(levelname)s - %(message)s\", level=logging.DEBUG)\r\n\r\n    logging.info(\"Starting to perform data collection...\")\r\n\r\n    load_dotenv()\r\n    SENHA = os.environ.get(\"Senha\")\r\n    EMAIL = os.environ.get(\"Email\")\r\n    INVESTING_PAGE_URL = \"https://br.investing.com/currencies/usd-brl-historical-data\"\r\n\r\n    options = Options()\r\n    options.binary_location = os.environ.get(\"GOOGLE_CHROME_BIN\")\r\n    options.add_argument(\"--headless\")\r\n    options.add_argument(\"--no-sandbox\")\r\n    options.add_argument(\"--disable-dev-shm-usage\")\r\n    options.add_argument(\"window-size=1400x800\")\r\n    options.add_argument(\"--disable-gpu\")\r\n    options.add_argument(\"--remote-debugging-port=9222\")\r\n    options.add_argument(\"start-maximized\")\r\n    logging.info(\"Webdriver iniciando\")\r\n    driver = webdriver.Chrome(\r\n        service=Service(ChromeDriverManager().install()), options=options\r\n    )\r\n\r\n    driver.get(INVESTING_PAGE_URL)\r\n    time.sleep(10)\r\n\r\n    WebDriverWait(driver, 20).until(\r\n        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"userAccount\"]/div/a[1]'))\r\n    ).click()\r\n    time.sleep(5)\r\n\r\n    input_element = driver.find_element_by_xpath('//*[@id=\"loginFormUser_email\"]')\r\n    input_element.send_keys(EMAIL)\r\n    input_element = driver.find_element_by_xpath('//*[@id=\"loginForm_password\"]')\r\n    input_element.send_keys(SENHA)\r\n\r\n    WebDriverWait(driver, 20).until(\r\n        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"signup\"]/a'))\r\n    ).click()\r\n    time.sleep(20)\r\n\r\n    select = Select(driver.find_element_by_xpath('//*[@id=\"data_interval\"]'))\r\n    select.select_by_visible_text(\"Mensal\")\r\n\r\n    WebDriverWait(driver, 5).until(\r\n        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"widgetFieldDateRange\"]'))\r\n    ).click()\r\n    time.sleep(5)\r\n\r\n    input_element = driver.find_element_by_xpath('//*[@id=\"startDate\"]')\r\n    input_element.clear()\r\n    input_element.send_keys(\"31/12/1994\")\r\n\r\n    today = date.today()\r\n    d1 = today.strftime(\"%d/%m/%Y\")\r\n    input_element = driver.find_element_by_xpath('//*[@id=\"endDate\"]')\r\n    input_element.clear()\r\n    input_element.send_keys(d1)\r\n\r\n    WebDriverWait(driver, 5).until(\r\n        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"applyBtn\"]'))\r\n    ).click()\r\n    time.sleep(5)\r\n\r\n    body = driver.find_elements(By.CSS_SELECTOR, \"#curr_table\")\r\n    for i in body:\r\n        get = pd.Series(i.text)\r\n\r\n    driver.close()\r\n    driver.quit()\r\n\r\n    logging.info(\"Starting data storing process...\")\r\n\r\n    names = get[0].split(\"\\n\")[0].split(\" \")\r\n    headers = [unidecode.unidecode(i) for i in names]\r\n\r\n    def join_str(pd_series: pd.Series) -> List[List[str]]:\r\n        entries = pd_series[0].split(\"\\n\")[1:]\r\n        entries_list = [i.split(\" \") for i in entries]\r\n        for i, _ in enumerate(entries_list):\r\n            entries_list[i][0:2] = [\" \".join(entries_list[i][0:2])]\r\n\r\n        return entries_list\r\n\r\n    logging.info(\"Data collection finished.\")\r\n\r\n    final_data = pd.DataFrame(join_str(get), columns=headers)\r\n    disk_engine = create_engine(\"sqlite:///usd_brl.db\")\r\n\r\n    def write_to_disk(df):\r\n        df.to_sql(\"usd_brl\", disk_engine, if_exists=\"append\", index=False)\r\n\r\n    write_to_disk(final_data)\r\n\r\n    logging.info(\"Data stored successfully.\")\r\n\r\nEtapa 2\r\nEssa talvez seja a etapa mais simples, por ter como objetivo apenas\r\ndefinir as funções de chamada do código definido previamente e o\r\ncarregamento das bases de dados em formato SQL:\r\n\r\n#load_data.py\r\n\r\nimport logging\r\nimport os\r\n\r\nimport pandas as pd\r\n\r\nimport get_table_pmi as get_table_pmi\r\nimport usd_brl as usd_brl\r\n\r\nlogging.basicConfig(format=\"%(levelname)s - %(message)s\", level=logging.DEBUG)\r\n\r\ntry:\r\n    os.remove(\"pmi.db\")\r\n    os.remove(\"usd_brl.db\")\r\nexcept FileNotFoundError:\r\n    pass\r\n\r\n\r\ndef get_pmi():\r\n    get_table_pmi.get_data_pmi()\r\n    data_pmi = pd.read_sql(\"SELECT * FROM pmi\", \"sqlite:///pmi.db\")\r\n    logging.info(\"BSI created and loaded successfully.\")\r\n    print(data_pmi)\r\n    return data_pmi\r\n\r\n\r\ndef get_currency():\r\n    usd_brl.get_data_usd_brl()\r\n    data_usd_brl = pd.read_sql(\"SELECT * FROM usd_brl\", \"sqlite:///usd_brl.db\")\r\n    logging.info(\"USD-BRL created and loaded successfully.\")\r\n    print(data_usd_brl)\r\n    return data_usd_brl\r\n\r\nEtapa 3\r\nEssa etapa é um pouco mais complexa por envolver a obtenção direta\r\ndas bases chamando os códigos anteriores, como também realizar a\r\nprodução dos gráficos e salvá-los em formato html e\r\nperformar um pouco de data wrangling:\r\n\r\n#plots_monitor.py\r\n\r\ndef plots():\r\n    import datetime\r\n\r\n    import pandas as pd\r\n    import plotly.graph_objects as go\r\n    from plotly.subplots import make_subplots\r\n\r\n    import load_data\r\n\r\n    data_pmi = load_data.get_pmi()\r\n    data_usd_brl = load_data.get_currency()\r\n\r\n    data_pmi[\"fluctuation\"] = data_pmi[\"previous\"] - data_pmi[\"actual\"]\r\n    data_pmi.drop_duplicates(subset=[\"date\"], inplace=True)\r\n\r\n    # _______________________________________________________________________\r\n    data_usd_brl = data_usd_brl.apply(lambda x: x.str.replace(\",\", \".\"))\r\n\r\n    list_1 = [\r\n        \"Jan\",\r\n        \"Fev\",\r\n        \"Mar\",\r\n        \"Abr\",\r\n        \"Mai\",\r\n        \"Jun\",\r\n        \"Jul\",\r\n        \"Ago\",\r\n        \"Set\",\r\n        \"Out\",\r\n        \"Nov\",\r\n        \"Dez\",\r\n    ]\r\n    list_2 = [\r\n        \"Jan\",\r\n        \"Feb\",\r\n        \"Mar\",\r\n        \"Apr\",\r\n        \"May\",\r\n        \"Jun\",\r\n        \"Jul\",\r\n        \"Aug\",\r\n        \"Sep\",\r\n        \"Oct\",\r\n        \"Nov\",\r\n        \"Dec\",\r\n    ]\r\n\r\n    for i, _ in enumerate(list_1):\r\n        data_usd_brl[\"Data\"] = data_usd_brl[\"Data\"].str.replace(list_1[i], list_2[i])\r\n\r\n    data_usd_brl[\"fluctuation\"] = data_usd_brl[\"Ultimo\"].astype(float).diff(1)\r\n\r\n    data_usd_brl.drop_duplicates(subset=[\"Data\"], inplace=True)\r\n\r\n    for i, _ in enumerate(data_usd_brl[\"Data\"]):\r\n        data_usd_brl[\"Data\"][i] = datetime.datetime.strptime(\r\n            data_usd_brl[\"Data\"][i], \"%b %y\"\r\n        ).strftime(\"%Y-%m-%d\")\r\n\r\n    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"BCI\", \"Dólar - Real\"))\r\n\r\n    fig.add_trace(\r\n        go.Waterfall(\r\n            name=\"20\",\r\n            orientation=\"v\",\r\n            measure=[\"relative\"],\r\n            x=data_pmi[\"date\"],\r\n            textposition=\"outside\",\r\n            y=data_pmi[\"fluctuation\"],\r\n            base=data_pmi[\"actual\"].iloc[1],\r\n            increasing={\"marker\": {\"color\": \"rgb(217, 17, 57)\"}},\r\n            decreasing={\"marker\": {\"color\": \"rgb(7, 181, 117)\"}},\r\n            connector={\"line\": {\"color\": \"rgb(63, 63, 63)\"}},\r\n        ),\r\n        row=1,\r\n        col=1,\r\n    )\r\n\r\n    fig.add_trace(\r\n        go.Waterfall(\r\n            name=\"20\",\r\n            orientation=\"v\",\r\n            measure=[\"relative\"],\r\n            x=data_usd_brl[\"Data\"],\r\n            textposition=\"outside\",\r\n            y=data_usd_brl[\"fluctuation\"],\r\n            base=data_usd_brl[\"Ultimo\"].astype(float).iloc[0],\r\n            decreasing={\"marker\": {\"color\": \"rgb(217, 17, 57)\"}},\r\n            increasing={\"marker\": {\"color\": \"rgb(7, 181, 117)\"}},\r\n            connector={\"line\": {\"color\": \"rgb(63, 63, 63)\"}},\r\n        ),\r\n        row=1,\r\n        col=2,\r\n    )\r\n\r\n    fig.update_layout(\r\n        title=\"Índices Monitorados - PMI e cotação USD\",\r\n        title_font_family=\"Droid Sans Mono\",\r\n        title_font_size=20,\r\n        showlegend=False,\r\n    )\r\n\r\n    fig.update_xaxes(title_text=\"Data\")\r\n    fig.update_yaxes(\r\n        title_text=\"Índice de Gerentes de Compras (PMI) da Caixin\", col=1, row=1\r\n    )\r\n    fig.update_yaxes(\r\n        title_text=\"USD/BRL - Dólar Americano - Real Brasileiro\", col=2, row=1\r\n    )\r\n    fig.write_html(\"plots.html\")\r\n    return fig\r\n\r\nEtapa 4\r\nEm seguida é definido o módulo com a função para chamado da API do\r\nTelegram. Para isso é necessária ainda a criação de um Bot no aplicativo\r\nseguindo este tutorial.\r\nEsse Bot então possui a função de receber as atualizações mensais sobre\r\nos índices tanto em formato textual send_msg quanto em\r\nformato de imagem send_plots:\r\n\r\n#send_msg.py\r\n\r\n\r\ndef send():\r\n\r\n    import os\r\n    from datetime import date\r\n\r\n    import pandas as pd\r\n\r\n    import plots_monitor as plots_monitor\r\n\r\n    plots_monitor.plots()\r\n\r\n    data_usd_brl = pd.read_sql(\"SELECT * FROM usd_brl\", \"sqlite:///usd_brl.db\")\r\n    data_pmi = pd.read_sql(\"SELECT * FROM pmi\", \"sqlite:///pmi.db\")\r\n\r\n    def send_msg(text: str):\r\n        import dotenv\r\n        import requests\r\n\r\n        dotenv.load_dotenv()\r\n\r\n        token = os.environ.get(\"token\")\r\n        userID = \"2078337734\"\r\n        message = text\r\n\r\n        # Create url\r\n        url = f\"https://api.telegram.org/bot{token}/sendMessage\"\r\n\r\n        # Create json link with message\r\n        data = {\"chat_id\": userID, \"text\": message}\r\n\r\n        # POST the message\r\n        requests.post(url, data)\r\n\r\n    def send_plots():\r\n        import dotenv\r\n        import requests\r\n\r\n        dotenv.load_dotenv()\r\n\r\n        token = os.environ.get(\"token\")\r\n        userID = \"2078337734\"\r\n\r\n        # Create url\r\n        url = f\"https://api.telegram.org/bot{token}/sendDocument\"\r\n\r\n        # Create json link with message\r\n        data = {\"chat_id\": userID, \"document\": \"attach://file\"}\r\n        file = {\"file\": open(\"plots.html\", \"rb\")}\r\n\r\n        # POST the message\r\n        requests.post(url, data, files=file)\r\n\r\n    dolar_actual = float(data_usd_brl[\"Ultimo\"][0].replace(\",\", \".\"))\r\n\r\n    send_plots()\r\n\r\n    send_msg(\r\n        f\"\"\"{date.today()}: Notificação de índices:\r\n    Dólar menor que 4,5: valor atual U${dolar_actual}\"\"\"\r\n    )\r\n\r\n    send_msg(\r\n        f\"\"\"{date.today()}: Notificação de índices:\r\n    PMI menor que 50%: valor atual {data_pmi[\"actual\"][1]}%\"\"\"\r\n    )\r\n\r\nEtapa 5\r\nEm seguida é criado o módulo main.py que será executado\r\npela linha de comando e traz toda a codebase produzida, além de proceder\r\ncom a realização do agendamento do código (toda quarta às 21:33 no\r\nhorário da Bahia).\r\n\r\n# main.py\r\n\r\nfrom apscheduler.schedulers.blocking import BlockingScheduler\r\nfrom pytz import timezone\r\n\r\nfrom send_msg import send\r\n\r\nsched = BlockingScheduler()\r\n\r\n\r\n@sched.scheduled_job(\r\n    \"cron\", day_of_week=\"wed\", hour=21, minute=33, timezone=\"America/Bahia\"\r\n)\r\ndef scheduled_job():\r\n    send()\r\n    print(\"This job is run monday at 20:00\")\r\n\r\n\r\nsched.start()\r\n\r\nEtapa 6\r\nAgora vamos a parte não tão legal da coisa que envolve a definição de\r\nalguns arquivos:\r\nProcfile: esse arquivo contém o comando a ser executado no\r\nterminal do nosso servidor no Heroku\r\nclock: python src/de_project/main.py, além de trazer no seu\r\niníco a definição de qual tipo de trabalho será executado\r\n(clock) indicando que é um agendamento de ação.\r\nsetup.sh: esse arquivo traz o seguinte comando que indica algumas\r\nfunções a serem performadas pelo servidor:\r\nmkdir -p ~/.streamlit/\r\n\r\necho \"\\\r\n[server]\\n\\\r\nheadless = true\\n\\\r\nport = $PORT\\n\\\r\nenableCORS = false\\n\\\r\n\\n\\\r\n\" > ~/.streamlit/config.toml\r\nruntime.txt: esse arquivo define qual a versão do Python a ser\r\nexecutada.\r\nrequirements.txt: aqui constam as dependências do projeto\r\n(bibliotecas usadas);\r\nOs demais arquivos podem ser ignorados.\r\nEtapa 7\r\nEssa é a mais crucial pois envolve alguns passos para enviar esse\r\nprojeto aos sites do github e do Heroku. Ela pode ser realizada usando\r\neste\r\ne este\r\nmateriais.\r\n1 - Criar uma conta no Github;\r\n2 - Enviar o projeto ao Github;\r\n3 - Criar uma conta no Heroku;\r\n4 - Instalar o Heroku CLI para a obtenção do programa na linha de\r\ncomando:\r\n5 - Criar uma aplicação no site do Heroku;\r\n6 - Ir à página de Settings dessa aplicação no Heroku e adicionar as\r\nvariáveis do Ambiente que foi definida no código (Email, Senha,\r\nToken);\r\n7 - Adicionar também o Chrome e o Chrome Drive nas variáveis do ambiente\r\nquanto nas buildpacks;\r\n8 - Em seguida realizar o envio desse projeto usando a CLI do Git e do\r\nHeroku para o deploy da aplicação.\r\n9 - Por fim, após envio do projeto à branch do Heroku, executar o\r\ncomando heroku ps:scale clock=1 no terminal da sua máquina\r\npara efetivamente definir a agenda de execução.\r\nPara saber mais sobre o agendamento do programa no Heroku, basta\r\nacessar este link.\r\nResultados:\r\nO resultado final desse projeto é o recebimento no Telegram tanto da\r\nmensagem dizendo a cotação atual dos índices quanto um arquivo html\r\ninterativo para análise da série histórica.\r\nGráficos com as séries históricas do\r\nÍndice PMI e da cotação Dólar-Real.Mensagem no Bot do Telegram com a\r\nnotificação da cotação dos índices.\r\n\r\n\r\n",
    "preview": "posts/2022-06-08-automao-de-raspagem-de-dados/featured.jpg",
    "last_modified": "2022-06-08T23:54:45-03:00",
    "input_file": "automao-de-raspagem-de-dados-usando-selenium-e-heroku.knit.md"
  },
  {
    "path": "posts/2022-05-12-obtendo-coordenadas-a-partir-de-pdf/",
    "title": "Obtendo coordenadas a partir de PDF",
    "description": "Como obter dados de localização espacial a partir de texto.",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2022-05-12",
    "categories": [],
    "contents": "\r\nObtendo as coordenadas\r\nNo fim do último ano, recebi uma demanda para obter o perímetro das\r\náreas de uma Unidade de Conservação (UC) localizada na região Sul da\r\nBahia para verificação de possíveis sobreposições com locais a serem\r\nlicenciados.\r\nQuem lida com dados geoespaciais de UCs sabe que essa é uma tarefa\r\nrelativamente simples. Porém, também foi solicitado o perímetro da Zona\r\nde Amortecimento (ZA) desse local. Aí que tivemos uma complicação, pois\r\nnão pude achar nenhum material em formato vetorial com esse limite. A\r\nsaída foi obter o decreto\r\nde criação dessa UC e extrair os dados de posicionamento. E aí foi que o\r\nbarraco desabou: eram mais de duas centenas de coordenadas. Extrair essa\r\nquantidade manualmente seria complexo e foi então que decidi usar o\r\nR.\r\nRequisitos para extração de\r\ndados\r\nPara fazer essa obtenção, organização e plotagem de dados a partir do\r\ntexto do decreto em formato PDF foram necessárias algumas\r\nbibliotecas:\r\n\r\n\r\nlibrary(tidyverse) # conjunto de libs para manipulação de dados\r\nlibrary(here) # lib para definir tornar a obtenção de arquivos mias fácil\r\nlibrary(pdftools) # lib para ler o texto do .pdf\r\nlibrary(data.table) # lib para tabelar as coordenadas\r\nlibrary(sf) # lib para manipular arquivos vetorias\r\nlibrary(mapview) # lib para plotar os dados em um mapa interativo\r\n\r\n\r\n\r\nExtraindo os dados\r\nA aquisição de dados foi feita inicialmente com a leitura do arquivo\r\ne obtenção de 4 páginas de texto usando a lib {pdftools}:\r\n\r\n\r\ndecreto <- pdftools::pdf_text(here(\"decreto\", \"decreto.pdf\"))\r\n\r\n\r\n\r\nEm seguida, foi criada uma função get_coords usando\r\nexpressões regulares (regex) para obtenção de partes do texto que\r\nseguisse o seguinte padrão:\r\n1 - obter valores que começam com 8, seguido de uma série de números\r\ne possuem mais 6 digitos (lat)\r\n2 - obter valores que começam com 3 ou 4, seguido de uma série de\r\nnúmeros e possuem mais 5 digitos (long)\r\nA função ainda realiza a obtenção das coordenadas a partir de um\r\níndice (index) e as converte em valores numéricos, em\r\nseguida as salvando em uma coluna de tibble (tabela) que será\r\nunificada:\r\n\r\n\r\nget_coords <- function(pdf, index){\r\n  long <- tibble(long = str_extract_all(pdf, \"8[0-9]{6}\")[[index]] %>% as.numeric())\r\n  \r\n  lat <- tibble(lat = str_extract_all(pdf, \"[3-4][0-9]{5}\")[[index]] %>% as.numeric()) %>% \r\n    filter(lat != 409448 & lat != 400074)\r\n  \r\n  long %>% bind_cols(lat)\r\n  \r\n}\r\n\r\n\r\n\r\nEntão a função foi aplicada sobre as 4 páginas de texto obtidas\r\nusando a função map e as 4 tibbles obtidas então foram\r\nunificadas em uma única datatable com a função rbindlist.\r\nEm seguida, foi adicionada a coluna id contendo o número de\r\ncada linha da data.table.\r\nComo a leitura das páginas foi feita de forma geral, foi necessário\r\nidentificar qual base de dados correspondia às coordenadas da UC e qual\r\ncorrespondia às da ZA, para isso o documento foi avaliado e foi\r\nidentificada que a quebra acontecia na coordenada de número 224. Após a\r\nseparação das séries de coordenadas, foi percebido que um dos pontos do\r\nperímetro da Zona de Amortecimento tinha um erro de digitação. Então\r\nesse erro teve de ser corrigido de forma manual, sendo a coordenada\r\nadicionada à tabela de pontos:\r\n\r\n\r\n1:4 %>% \r\n  purrr::map(get_coords, pdf = decreto) %>%\r\n  data.table::rbindlist() -> my_df\r\n\r\nmy_df <- my_df %>% \r\n  mutate(id = 1:nrow(my_df))\r\n\r\nmy_df %>% filter(lat == 390724)\r\n\r\nmy_df_uc <- my_df %>% \r\n  filter(id < 224) %>% \r\n  select(-id)\r\n\r\nmy_df_za <- my_df %>% \r\n  filter(id >= 224) %>% \r\n  select(-id) %>% \r\n  bind_rows(c(long = 8181041, lat = 409448))\r\n\r\n\r\n\r\nPlotagem dos dados\r\nAgora vem a parte divertida da coisa: plotagem e análise visual.\r\nAssim, primeiro foi definido o sistema de projeção dos pontos a serem\r\nusados (Sirgas 2000 UTM Zone 24S). Em seguida as tabelas de pontos foram\r\nconvertidas em objetos sf (simple feature) para facilitar a\r\nsua manipulaçção e plotagem pela lib {sf}. Então, para comparar os\r\ndados, foi carregado o arquivo .shp oficial da UC em questão e feita a\r\nsua reprojeção espacial. Por fim, as três feições (pontos da UC, ZA e a\r\nárea oficial da UC) foram unificadas em uma lista e plotadas em um mapa\r\ninterativo usando a lib {mapview}:\r\n\r\n\r\nproj <- \"+proj=utm +zone=24 +south +ellps=aust_SA +towgs84=-57,1,-41,0,0,0,0 +units=m +no_defs\"\r\n\r\npoints_uc <- st_as_sf(x = my_df_uc, \r\n                   coords = c(\"lat\", \"long\"), \r\n                   crs = proj)\r\n\r\npoints_za <- st_as_sf(x = my_df_za, \r\n                      coords = c(\"lat\", \"long\"), \r\n                      crs = proj)\r\n\r\narea_uc <- read_sf(here(\"ucs\", \"ucstodas.shp\")) %>% \r\n  filter(NOME_UC1 == \"PARQUE NACIONAL DO ALTO CARIRI\") %>% \r\n  st_transform(crs = st_crs(proj))\r\n\r\nmapview(list(points_uc, points_za, area_uc), color = c(\"red\", \"green\", \"white\"))\r\n\r\n\r\n\r\nÁreas do Parque Nacional do Alto Cariri,\r\nna Bahia.Conclusão\r\nEssa foi uma abordagem simples e que nos poupou bastante tempo.\r\nEspero que este material possa servir de apoio a pessoas que encontrem\r\nos mesmos desafios que tive. Até a próxima!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-12-obtendo-coordenadas-a-partir-de-pdf/featured.jpg",
    "last_modified": "2022-05-12T23:35:33-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-22-logging-Python/",
    "title": "Como usar logging em Python",
    "description": "Uma breve introdução ao uso de logging.",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2022-02-04",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Basemap com o local das coordendas informadas.\r\n\r\n\r\n\r\nPor qual razão usar logging?\r\nO uso de logging visa facilitar o monitoramento de eventos durante a execução de um programa. Isso pode ser feito a partir da expressão print(). Porém o módulo logging fornece capabilidades mais elegantes de rastrear os eventos dentro do programa em questão. Isso é feito adicionando chamadas de logging ao longo do código. Achou a coisa meio abstrata? Vamo ver na prática então.\r\nExemplo prático\r\nApenas recentemente tive contato com o módulo logging nos meus estudos de Python. Isso aconteceu quando estava realizando o desenvolvimento de um código pra utilizar no meu trabalho na área de geoprocessamento. Neste caso, vou mostrar como usar logging para apresentar os acontecimentos durante a criação de um basemap com os pontos do meu interesse. A seguir são importadas as libs necessárias:\r\n\r\nimport pandas as pd\r\nimport plotly.express as px\r\nimport utm\r\n\r\nimport logging\r\n\r\nA primeira coisa a ser feita é definir as configurações de logging. Neste caso, optei por definir a formatação do output para apresentar o nível de severidade dos eventos (neste caso, o mais baixo logging.DEBUG) e a mensagem definida na chamada. Aqui eventos de qualquer nível pode ser rastreado, podendo ser definido a escolha do desenvolvedor entre 5 níveis de severidade distintos (DEBUG, INFO, WARNING, ERROR e CRITICAL).\r\n\r\nlogging.basicConfig(format='%(levelname)s - %(message)s', level=logging.DEBUG)\r\n\r\nEm seguida, dei sequência ao desenvolvimento do programa. Neste caso, defini o Descriptor Protocol Coordinate para gerenciar as coordenadas geodésicas passadas como atributos à classe Point. Como coordendas são em pares (x, y), uma forma de evitar repetir o código de definição de getters e setters de ambos os valores foi a partir do uso de descriptors:\r\n\r\nclass Coordinate:\r\n    def __set_name__(self, owner, name):\r\n        self._name = name\r\n\r\n    def __get__(self, instance, owner):\r\n        return instance.__dict__[self._name]\r\n\r\n    def __set__(self, instance, value):\r\n        try:\r\n            instance.__dict__[self._name] = float(value)\r\n            logging.info('Validated values.')\r\n        except ValueError:\r\n            logging.exception(f'\"{self._name}\" must be a number')\r\n\r\nNeste caso, utilizei logging para monitorar se os valores de x e y passados ao descriptor Coordinate são valores válidos. Caso estes valores não sejam válidos, é levantado um erro e uma mensagem de logging. Mais sobre descriptors pode ser acessado aqui. Pronto! Esses foram os primeiros casos de uso de logging.\r\nEm seguida, defin a classe Point, em que a mesma recebe informações sobre coordenadas de um ponto em específico:\r\n\r\nclass Point:\r\n    x = Coordinate()\r\n    y = Coordinate()\r\n\r\n    def __init__(self, x, y, zone=None, northern=None):\r\n        self.x = x\r\n        self.y = y\r\n        self.zone = zone\r\n        self.northern = northern\r\n\r\n    def __str__(self):\r\n        if self.zone is not None and self.northern is True:\r\n            return f'Long: {self.x}m, Lat: {self.y}m, Zone: {self.zone}N'\r\n        elif self.zone is not None and self.northern is False:\r\n            return f'Long: {self.x}m, Lat: {self.y}m, Zone: {self.zone}S'\r\n        else:\r\n            return f'Lat: {self.x}°, Long: {self.y}°'\r\n\r\n    def __repr__(self):\r\n        if self.zone is not None and self.northern is True:\r\n            return f'Point({self.x}, {self.y}, {self.zone}, northern={self.northern})'\r\n        elif self.zone is not None and self.northern is False:\r\n            return f'Point({self.x}, {self.y}, {self.zone}, northern={self.northern})'\r\n        else:\r\n            return f'Point({self.x}, {self.y})'\r\n    \r\n    def plot_coord(self):        \r\n        if self.zone is None:\r\n            lat, long = self.x, self.y            \r\n        else:\r\n            logging.warning('Converting coordinates to UTM format.')\r\n            lat, long = utm.to_latlon(self.x, self.y, self.zone, northern=self.northern)\r\n            logging.warning('Conversion is completed.')\r\n            \r\n            \r\n        fig = px.scatter_mapbox(lat=pd.Series([lat]), lon=pd.Series([long]),\r\n                            color_discrete_sequence=[\"fuchsia\"], zoom=3, height=300)\r\n        fig.update_layout(mapbox_style=\"open-street-map\")\r\n        fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\r\n        fig.show()\r\n\r\nNesta classe primeiro são chamados os descriptos para compartilhar o comportamento das diferentes coordendas (podendo ser adicionada uma terceira z). Então a instância da classe é inicializada com os valores das coordendas (x e y) e com informações sobre a zona e o hemisfério. Estas duas últimas são importantes em situações onde os valores das coordenadas informadas são em UTM. Assim, pode-se ter duas formas de passar dos valores destes atributos.\r\nO próximo passo foi definir as formas de representação da classe com os dunder methods repr e str, também com validação de modo a apresentar o formato correto das coordenadas ao chamar uma instância da classe Point.\r\nE, por fim, é definido um método para plotar um basemap interativo com a localização do ponto em questão. O basemap é oriundo da lib plotly e recebe apenas valores em formato geodésico, sendo necessária a conversão de coordendas quando passadas em formato UTM. Antes de realizar a conversão usando a lib utm, são apresentados logging.warnings de que está sendo feita uma conversão das coordendas e que a conversão foi finalizada.\r\nTestando os resultados\r\nAgora resta realizar os testes e avaliar o uso da lib logging com coordendas em formato UTM:\r\n\r\n>>> utm_coords = Point(283979.44, 8451361.31, 24, False)\r\nINFO - Validated values.\r\nINFO - Validated values.\r\n\r\n>>> utm_coords\r\nPoint(283979.44, 8451361.31, 24, northern=False)\r\n\r\n>>> print(utm_coords)\r\nLong: 283979.44m, Lat: 8451361.31m, Zone: 24S\r\n\r\n>>> utm_coords.plot_coord() # O basemap é gerado\r\nWARNING - Converting coordinates to UTM format.\r\nWARNING - Conversion is completed.\r\n\r\nTestes com coordendas em formato geodésico:\r\n\r\n>>> geo_coords = Point(-14, -41)\r\nINFO - Validated values.\r\nINFO - Validated values.\r\n\r\n>>> geo_coords\r\nPoint(-14.0, -41.0) \r\n\r\n>>> print(geo_coords)      \r\nLat: -14.0°, Long: -41.0°  \r\n\r\n>>> geo_coords.plot_coord() # O basemap é gerado\r\n\r\nTestes com coordendas em formato “geodésico”errado”:\r\n\r\n>>> geo_coords.x = 'a'\r\nERROR - \"x\" must be a number\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 8, in __set__\r\nValueError: could not convert string to float: 'a'\r\n\r\nConclusão\r\nO uso de logging se mostrou bastante útil no desenvolvimento desse material, sendo uma das coisas mais legais que estudei até agora (a paixão por Python só aumenta). Muito obrigado pela visita e espero que tenham gostado da leitura!\r\nMais sobre logging pode ser encontrado nestas referências:https://realpython.com/python-logging/ https://www.youtube.com/watch?v=-ARI4Cz-awo&ab_channel=CoreySchafer (um dos melhores canais de Python)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-22-logging-Python/featured.png",
    "last_modified": "2022-02-04T00:05:05-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-20-predizendo-volume-de-eucalipto-com-tidymodels-xgboost-e-targets/",
    "title": "Predizendo volume de eucalipto com tidymodels, XGBoost e targets",
    "description": "\"Como montar um ambiente reprodutível para o ajuste de modelos de predição do volume de eucalipto.\"",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2021-10-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nFigure 1: Workflow do trabalho.\r\n\r\n\r\n\r\nUmas das dificuldades encontradas na academia é a reprodutibilidade de trabalhos. Essa é uma situação que, infelizmente, é corriqueira na área florestal. Neste texto, apresento algumas ferramentas que buscam melhorar a forma que trabalhos podem ser replicados, como a utilização dos pacotes here e renv.\r\nAproveitando o embalo, também mostro como usar o pacote tidymodels para a predição do volume de eucalipto. O dataset utilizado neste projeto é oriundo do excelente trabalho publicado no ano de 2020 pelo professor Gileno Azevedo juntamente com outros pesquisadores. O dataset pode ser encontrado na página do artigo. Vamos lá!\r\n1ª etapa - Iniciando o projeto\r\nAntes de qualquer coisa, preciso falar de 3 ferramentas básicas quando se busca reprodutibilidade em trabalhos no R:\r\nrenv\r\nEste pacote é um gerenciador de dependências: ele organiza e “memoriza” as dependências (como pacotes) que o seu projeto está usando de modo que, caso alguém refaça suas análises, não ocorra problemas como o uso de pacotes de versões distintas, além de tornar seu ambiente de trabalho isolado. Mais à frente explicarei outra importante vantagem. Caso queira saber mais sobre o renv, esse tutorial do Chaoran é show.\r\nR Projects\r\nÉ o mais simples dessa lista. Basicamente toda vez que você for criar um projeto, o ideal é que seja criada uma pasta principal e dentro dela serão criadas as demais pastas (data, R, etc.). Em seguida, abra o RStudio e crie um Project dentro desta pasta principal. A organização destas pastas terá a seguinte estrutura (a mesma utilizada neste trabalho):\r\n\r\n\r\n# Eucalipto_Volume (pasta principal)  \r\n#      |--Eucalipto_Volume.Rproj\r\n#      |--R  \r\n#      |--plots\r\n#      |--report\r\n\r\n\r\n\r\nhere\r\nEsse aqui é o mais legal: basicamente ele te permite criar caminhos relativos. Por ex.: ao invés de se referir a um arquivo como dados <- C:user/zezin/uma/duas/dados/meus_dados.csv, ou usar setwd(\"C:user/zezin/uma/duas/dados\"), prefira salvar seu Project em uma pasta principal e se referir a ele como here::here(\"dados\", \"meus_dados.csv\"). Isso facilita que outras pessoas reproduza suas análises sem grandes problemas. Tem um pessoal que recomenda fortemente usar o pacote here. Basicamente ele define o caminho de acesso aos arquivos a partir da última pasta do caminho (top-level folder), que no nosso exemplo hipotético é a pasta “dados”.\r\nMão na massa\r\nA primeira coisa que fiz foi criar uma pasta principal. Em seguida, criei dentro dela um R Project com o nome do meu projeto (Eucalipto_Volume). Então instalei o renv e executei o comando renv::init() para criar um ambiente local do projeto. A partir daí, iniciei as análises e instalei os pacotes básicos iniciais necessários (incluindo o here). Sempre que instalava um novo pacote, chamava renv::snapshot() para atualizar os estado do projeto e as dependências. Caso fosse necessário reverter alguma alteração de dependência no projeto após chamar renv::snapshot(), era acionado o comando renv::restore(). Pronto! Meu ambiente local do projeto está montado e isolado. Bacana, né?\r\n2ª etapa - Modelagem\r\nAgora a parte legal: a modelagem dos dados! Recentemente tem ganhado bastante tração um pacote de modelagem do R baseado na filosofia do tidyverse: o tidymodels. Esse pacote é incrível, permitindo usar diversos modelos de machine learning de forma extremamente intuitiva e bem elegante. A seguir vou mostrar o passo a passo necessário para ajustar e testar diversos modelos visando a predição do volume de eucalipto.\r\nCarregando os pacotes necessários:\r\n\r\n\r\nlibrary(here)\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\nlibrary(janitor)\r\nlibrary(EnvStats)\r\nlibrary(bestNormalize)\r\nlibrary(finetune)\r\nlibrary(doParallel)\r\nlibrary(extrafont)\r\n\r\n\r\n\r\nAquisição dos dados\r\nPrimeiramente serão adquiridas as bases de dados (treino e teste) a serem utilizadas no projeto a partir do site que disponibiliza o artigo e os dados. Aqui será criada uma pasta “data” onde os arquivos da base de dados serão baixados.\r\n\r\n\r\n# Criando o diretório onde a base será alocada\r\ndir.create(\"data\")\r\n\r\n# Acesso ao material\r\nlink_train <- \"https://doi.org/10.1371/journal.pone.0238703.s007\"\r\nlink_test <- \"https://doi.org/10.1371/journal.pone.0238703.s008\"\r\n\r\n# Aqui é criado o caminho de acesso às bases de dados\r\ndest_folder_train <- here::here(\"data\", \"train.xlsx\")\r\ndest_folder_test <- here::here(\"data\", \"test.xlsx\")\r\n\r\n# Aquisição da base de teste e treino usadas pelos autores do trabalho\r\nutils::download.file(link_train, \r\n                     destfile = dest_folder_train, \r\n                     mode = \"wb\")\r\n\r\nutils::download.file(link_test, \r\n                     destfile = dest_folder_test, \r\n                     mode = \"wb\")\r\n\r\n\r\n\r\nPerceba que aqui é usado o pacote here para a criação de um caminho relativo até a pasta de destino, em que o caminho até a pasta objetivo (neste caso, a pasta “data”) se inicia a partir da pasta principal criada para o projeto (Eucalipto_Volume), e não do seu diretório base. Caso a pasta principal do projeto seja alterada para outra partição, não será necessário chamar setwd() com um novo caminho.\r\nE qual a vantagem disso? Isso permite que você ou qualquer outra pessoa não precise alterar o caminho caso queira carregar/salvar as informações obtidas das mesmas análises aqui apresentadas. A utilização desse pacote é uma boa prática quando falamos de project-oriented workflows (workflow montado tendo como base projects).\r\nPreparando os dados\r\nAs bases de dados foram separadas originalmente pelos autores em dois arquivos. Essa é uma boa prática para criação de modelos e sua subsequente avaliação. Dividir os sets dessa forma permite que não ocorra data leakage (quando seu modelo, na fase de treino, “entra em contato” com os dados de teste, causando overfitting). Entretanto, uma vantagem do tidymodels é que ele permite o split entre set de treino e set de teste de forma unificada, sendo geradas partições de um mesmo arquivo sem a ocorrência de data leakage. Para ilustrar essa capacidade, os datasets serão unidos e então será mostrado como realizar a sua separação em treino e teste com o tidymodels:\r\n\r\n\r\n# Carregando a base de daoos separada\r\nsheet_train <- readxl::read_excel(here::here(\"data\", \"train.xlsx\"), \r\n                                  skip = 1)\r\nsheet_test <- readxl::read_excel(here::here(\"data\", \"test.xlsx\"), \r\n                                 skip = 1)\r\n\r\n# Unificando a base e \"limpando os nomes\" das colunas\r\ndataset <- \r\n    dplyr::bind_rows(sheet_train, sheet_test) |>\r\n    janitor::clean_names() |> \r\n    dplyr::mutate(across(c(stem:rotation), \r\n                         forcats::as_factor),\r\n                  across(c(tx, d), as.integer))\r\n\r\n\r\n\r\nIniciando o processo de modelagem\r\nApós unificar os dados, vamos colocar a mão na massa! A primeira coisa a ser feita é dividir os dados em treino e teste e então realizar a reamostragem repetida do set de treino para validação dos modelos. O tidymodels faz isso de forma prática:\r\n\r\n\r\nset.seed(1504)\r\n\r\nvol_split <- rsample::initial_split(data = dataset, \r\n                                    prop = 0.75, \r\n                                    strata = cod_volume)\r\ntrain_split <- rsample::training(vol_split)\r\ntest_split <- rsample::testing(vol_split)\r\n\r\n# Realizando validação cruzada repetida para avaliação dos modelos\r\nvol_folds <- rsample::vfold_cv(data = train_split, \r\n                               strata = cod_volume, \r\n                               repeats = 5)\r\n\r\n\r\n\r\nAlgumas considerações nessa etapa:\r\nO split inicial foi realizado usando uma proporção de 3/4.\r\nA divisão foi estratificada de acordo com as classes de volume comercial.\r\nO tidymodels permite a divisão do dataset de forma simples e que não ocorra data leakage, dispensando a necessidade de separar os dados manualmente.\r\nExplorando os dados\r\nNeste caso, temos um set de dados com poucos preditores: dbh (DAP) e h (Altura). Neste dataset também constam as variáveis tx e d que dizem respeito a se o volume de madeira predito é com casca ou sem casca (0 ou 1) e o diâmetro comercial, respectivamente. De modo a observar os padrões dos dados, vamos fazer uma análise exploratória básica das variáveis:\r\n\r\n\r\nggplot(data = train_split) +\r\n    geom_density(mapping = aes(x = dbh)) +\r\n    labs(x = \"DAP\")\r\n\r\nggplot(data = train_split) +\r\n    geom_density(mapping = aes(x = h)) +\r\n    labs(x = \"Altura\")\r\n\r\n# Explorando as relações entre as variáveis principais\r\nggplot(data = train_split) +\r\n    stat_count(mapping = aes(x = cod_volume))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = dbh, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = dbh^2, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = h^2, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = h, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = dbh*h, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = (dbh*h)^2, y = v))\r\n\r\nggplot(data = train_split) +\r\n    geom_point(mapping = aes(x = dbh, y = h))\r\n\r\n\r\n\r\nNeste caso, é recomendável normalizar os dados antes de seguir com a modelagem. Sem a padronização, uma variável pode ter um maior impacto sobre a resposta apenas por conta da sua escala, o que pode ser o caso aqui dadas as próprias unidades (cm e m) dos dados Essa normalização será aplicada mais à frente na modelagem.\r\nPode-se perceber também que a adição de interações e de termos quadráticos tornam a relação entre os preditores, especificamente dap e h, mais linear.\r\nTambém é possível observar que as classes de volume não possuem a mesma quantidade de observações. Neste caso, pode-se seguir com duas estratégias: a primeira seria ajustar um modelo para cada um dos diferentes volumes em separado; a segunda consiste na divisão ponderada (estratificação) no split dos dados (o treino e o teste teriam a mesma proporção de cada classe de volume). Neste trabalho, seguiremos a segunda estratégia.\r\nAjustando os modelos\r\nAgora vem a parte onde a mágica do tidymodels acontece: Primeiro é definida uma recipe do passo-a-passo que os dados devem ser processados. Em seguida são definidas as especificações modelos a serem utilizados. Serão definidas duas recipes (receitas de passo-a-passo): uma sem pré-processamento e outra pré-processada (com adião de interações, termos quadráticos e normalização):\r\n\r\n\r\n# Definindo os pré-processamentos dos dados\r\nsimple_vol_rec <- recipes::recipe(v ~ dbh + h + tx + d, data = train_split)\r\n\r\nnormalized_vol_rec <- simple_vol_rec |> \r\n  recipes::step_interact(terms = ~ dbh:h) |>  \r\n  recipes::step_mutate(dbh_sqrd = dbh^2, \r\n                       h_sqrd = h^2) |> \r\n  recipes::step_normalize(recipes::all_predictors(), -tx, -d)\r\n\r\n\r\n\r\nAqui pode-se perceber que a recipe normalized_vol_rec é apenas a recipe simple_vol_rec com mais “passos” adicionados.\r\nEm seguida, são definidos os diversos modelos a serem ajustados e depois testados. Estes modelos terão os hiperparâmetros mais importantes “marcados” para tunagem usando a função tune::tune.\r\n\r\n\r\n# Definindo diversos modelos\r\nlm_mod <- \r\n     parsnip::linear_reg() |> \r\n     parsnip::set_engine(\"lm\") |> \r\n     parsnip::set_mode(\"regression\")\r\n  \r\npenalized_lm_mod <- \r\n  parsnip::linear_reg(penalty = tune::tune(),\r\n                      mixture = tune::tune()) |> \r\n  parsnip::set_engine(\"glmnet\") |>\r\n  parsnip::set_mode(\"regression\")\r\n\r\nbag_mars_mod <- \r\n  baguette::bag_mars(prod_degree = tune::tune(), \r\n                     prune_method = \"exhaustive\",\r\n                     num_terms = tune::tune()) |> \r\n  parsnip::set_engine(\"earth\", times = 4) |> \r\n  parsnip::set_mode(\"regression\")\r\n  \r\ndec_tree <- \r\n  parsnip::decision_tree(cost_complexity = tune::tune(),\r\n                         tree_depth = tune::tune(),\r\n                         min_n = tune::tune()) |> \r\n  parsnip::set_engine(\"rpart\") |> \r\n  parsnip::set_mode(\"regression\")\r\n\r\nbag_cart_mod <- \r\n  baguette::bag_tree(cost_complexity = tune::tune(),\r\n                     tree_depth = tune::tune(),\r\n                     min_n = tune::tune()) |> \r\n  parsnip::set_engine(\"rpart\", times = 50L) |>\r\n  parsnip::set_mode(\"regression\")\r\n\r\nrf_spec <- \r\n  parsnip::rand_forest(mtry = tune::tune(), \r\n                       min_n = tune::tune(),\r\n                       trees = 1000) |> \r\n  parsnip::set_engine(\"ranger\") |> \r\n  parsnip::set_mode(\"regression\")\r\n  \r\nxgb_spec <- \r\n  parsnip::boost_tree(tree_depth = tune::tune(),\r\n                      learn_rate = tune::tune(),\r\n                      loss_reduction = tune::tune(),\r\n                      min_n = tune::tune(),\r\n                      sample_size = tune::tune(),\r\n                      mtry = tune::tune(),\r\n                      trees = 1000,\r\n                      stop_iter = 20) |> \r\n  parsnip::set_engine(\"xgboost\") |> \r\n  parsnip::set_mode(\"regression\")\r\n  \r\nsvm_r_spec <- \r\n  parsnip::svm_rbf(cost = tune::tune(),\r\n                   rbf_sigma = tune::tune(),\r\n                   margin = tune::tune()) |> \r\n    parsnip::set_engine(\"kernlab\") |> \r\n    parsnip::set_mode(\"regression\")\r\n  \r\nnnet_spec <- \r\n  parsnip::mlp(hidden_units = tune::tune(),\r\n               penalty = tune::tune(), \r\n               epochs = tune::tune()) |> \r\n    parsnip::set_engine(\"nnet\") |> \r\n    parsnip::set_mode(\"regression\")\r\n  \r\n  \r\nspecs_vol <- list(\"linear_reg\" = lm_mod, \r\n                  \"bag_mars\"= bag_mars_mod,\r\n                  \"decision_tree\" = dec_tree, \r\n                  \"bag_cart\" = bag_cart_mod, \r\n                  \"rf\" = rf_spec, \r\n                  \"xgb\" = xgb_spec,\r\n                  \"svm_rbf\" = svm_r_spec, \r\n                  \"nnet_mlp\" = nnet_spec,\r\n                  \"penalized_reg\" = penalized_lm_mod)\r\n\r\n\r\n\r\nTodas as especificações foram alocadas em uma lista que será usada mais à frente. Até aqui temos as duas recipes de pré-processamento a serem usadas nos dados e os 9 modelos especificados a serem tunados/ajustados.\r\nAgora vem a pergunta: como juntar tudo isso?\r\nEm um workflow (ou um workflowset, melhor dizendo).\r\n\r\n\r\n# Definindo o workflowset (serão ajustados 18 modelos ao todo)\r\nwflow_vol <- workflowsets::workflow_set(\r\n  models = specs_vol, #lista de modelos\r\n  preproc = list(\r\n    normalized = normalized_vol_rec,\r\n    simple = simple_vol_rec)\r\n  )\r\n\r\n\r\n\r\nCriado o workflowset, então será definida a configuração e o tipo de tunagem a ser aplicada. Neste caso, o processo de tunagem usado (race tuning) avalia todos os modelos em um set inicial da reamostragem. Baseado na performance corrente das métricas, alguns parâmetros que produzem modelos ruins, do ponto de vista preditivo, são então descartados na sequência do processo. Mais sobre esse processo de tunagem pode ser encontrado no livro do tidymodels.\r\n\r\n\r\n# Definindo as configurações da tunagem de parâmetros usando computação paralela\r\nracing_ctrl <- finetune::control_race(\r\n  save_pred = TRUE,\r\n  parallel_over = \"everything\",\r\n  save_workflow = TRUE\r\n)\r\n\r\n\r\n# Definindo a execução em paralelo \r\nclusters <- parallel::detectCores()\r\ncl <- parallel::makePSOCKcluster(clusters)\r\ndoParallel::registerDoParallel(cl)\r\n\r\n# Realizando a tunagem dos modelos\r\nresults_vol <-\r\n  workflowsets::workflow_map(\r\n    wflow_vol,\r\n    seed = 1504,\r\n    resamples = vol_folds,\r\n    control = racing_ctrl,\r\n    fn = \"tune_race_anova\",\r\n    grid = 25,\r\n    metrics = yardstick::metric_set(yardstick::rmse,\r\n                                    yardstick::rsq, \r\n                                    yardstick::huber_loss, \r\n                                    yardstick::mae)\r\n)\r\n# Parando a execução em paralelo \r\nparallel::stopCluster(cl)\r\nforeach::registerDoSEQ()\r\n\r\n\r\n\r\nApós realização da tunagem e escolhida a melhor configuração de parâmetros dos modelos de melhor desempenho, é feita a avaliação usando gráficos de acordo com as métricas definidas:\r\n\r\n\r\n# Plotagem dos resultados para cada métrica utilizada\r\nplot_results <- function(race_rslts, mtrc = \"rmse\",...){\r\n  workflowsets::autoplot(\r\n    race_rslts,\r\n    rank_metric = mtrc,  \r\n    metric = mtrc,       \r\n    select_best = TRUE,\r\n    ...\r\n    ) -> plot_racing\r\n  \r\n  ggplot2::ggsave(glue::glue(mtrc, \".png\"), \r\n                  path = here::here(\"plots\"))\r\n  graphics::plot(plot_racing)\r\n  \r\n}\r\n\r\nplot_results(results_vol, mtrc = \"rsq\")\r\nplot_results(results_vol, mtrc = \"rmse\")\r\nplot_results(results_vol, mtrc = \"huber_loss\")\r\nplot_results(results_vol, mtrc = \"mae\")\r\n\r\n\r\n\r\nAvaliação da qualidade dos modelos\r\nEm seguida, deve ser realizada a seleção do melhor modelo. Para isso, são criadas funções de:\r\nSeleção do melhor modelo.\r\nAjuste final do modelo selecionado.\r\nObtenção das métricas de avaliação do modelo selecionado no set de teste.\r\n\r\n\r\n# Função para seleção de modelos de acordo com o R² (padrão)\r\nselect_models <- function(grid_results, metric = \"rsq\", rank_posit = 1){\r\n  \r\n  workflowsets::rank_results(grid_results, \r\n                             select_best = TRUE) |> \r\n  dplyr::relocate(rank) |> \r\n  dplyr::select(-c(.config, model, std_err)) |> \r\n  dplyr::filter(.metric == \"rsq\" & rank == rank_posit) -> model_selected\r\n  EnvStats::print(model_selected)\r\n  \r\n}\r\n\r\n# Função para ajuste final do modelo\r\nfit_model <- function(grid_results, model_ranked, df_split, metric = \"rmse\"){\r\n    \r\n    name_model <- model_ranked |> purrr::pluck(\"wflow_id\", 1)\r\n  \r\n    model <- grid_results |> \r\n        workflowsets::extract_workflow_set_result(id = name_model) |>\r\n        tune::select_best(metric = metric)\r\n    \r\n    grid_results |> \r\n        workflowsets::extract_workflow(name_model) |>\r\n        tune::finalize_workflow(model) |> \r\n        tune::last_fit(split = df_split,\r\n                       metrics = yardstick::metric_set(yardstick::rmse, \r\n                                                       yardstick::rsq, \r\n                                                       yardstick::huber_loss, \r\n                                                       yardstick::mae))\r\n    \r\n}\r\n\r\n# Função para obtenção da performance do melhor modelo no set de teste\r\nmetrics_mod <- function(best_mod, model_ranked){\r\n  \r\n  name_model <- model_ranked |> purrr::pluck(\"wflow_id\", 1)\r\n  \r\n  workflowsets::collect_metrics(best_mod) |> \r\n    kableExtra::kbl(caption = glue::glue(\"Performance do modelo \", \r\n                                         name_model)) |> \r\n    kableExtra::kable_classic(full_width = F, \r\n                              html_font = \"Cambria\", \r\n                              font_size = 16) |> \r\n    kableExtra::save_kable(file = here::here(\"plots\", \r\n                                             glue::glue(\"perf_\", \r\n                                                        name_model, \r\n                                                        \".png\")),\r\n                           self_contained = T)\r\n  \r\n  workflowsets::collect_metrics(best_mod)\r\n}\r\n\r\n# Seleção, ajuste e avaliação do melhor modelo\r\nbest_model <- select_models(grid_results = results_vol)\r\n\r\nfit_best_mod <- fit_model(grid_results = results_vol,\r\n                        model_ranked = best_model,\r\n                        df_split = vol_split)\r\n\r\nmetrics_mod(fit_best_mod, best_model)\r\n\r\n\r\n\r\nFigure 2 - Desempenho dos modelos.De modo a termos um ponto de comparação didático, serão comparados um modelo de qualidade intermediária e o melhor modelo. Esse processo se dá de forma similar ao realizado previamente, exceto que agora será selecionado o modelo ranqueado na posição número 10 (rank_posit = 1) em contraste ao melhor modelo (rank_posit = 1):\r\n\r\n\r\n# Seleção, ajuste e avaliação do modelo intermediário\r\nintermediary_model <- select_models(grid_results = results_vol, rank_posit = 10)\r\n\r\nfit_intermediary_model <- fit_model(grid_results = results_vol,\r\n                                       model_ranked = intermediary_model,\r\n                                       df_split = vol_split)\r\n\r\nmetrics_mod(fit_intermediary_model, intermediary_model)\r\n\r\n\r\n\r\nAgora ambos os modelos ajustados previamente serão comparados através de gráficos de dispersão contendo o volume predito vs volume observado, permitindo uma análise visual da qualidade preditiva de cada um. Para isso serão definidas duas funções para geração do gráfico de dispersão de cada modelo.\r\n\r\n\r\n# Função para plotar os resultados dos modelos\r\nscatterplot <- function(.x, .y){\r\n    \r\n    ggplot2::ggplot(.x) + \r\n        ggplot2::geom_point(mapping = ggplot2::aes(x = .pred, y = v)) + \r\n        ggplot2::geom_point(mapping = ggplot2::aes(x = .pred, y = v), \r\n                   alpha = 0.5, \r\n                   size = 2, \r\n                   color = \"black\", \r\n                   fill = \"mediumspringgreen\", \r\n                   pch = 21) + \r\n        ggplot2::geom_abline(lty = 2, \r\n                    col = \"black\", \r\n                    size = 1) +\r\n        tune::coord_obs_pred() +\r\n        ggplot2::theme_classic() +\r\n        ggplot2::theme(text = ggplot2::element_text(family = \"Source Code Pro\")) +\r\n        ggplot2::ggtitle(.y) +\r\n        ggplot2::labs(x = \"Volume Predito (m³)\",\r\n                      y = \"Volume Observado (m³)\") -> plot_scatterplot\r\n    \r\n  ggplot2::ggsave(glue::glue(.y, \".png\"), path = here::here(\"plots\"))\r\n  graphics::plot(plot_scatterplot)\r\n  \r\n}\r\n\r\n# Função para extrair os dados preditos e plotá-los vs os \r\n# dados observados usando a função anterior\r\naccessing_models <- function(mod1, mod2, model_ranked1, model_ranked2){\r\n    \r\n    gsub_und <- function(x) gsub(\"_\", \" \", x)\r\n    name_model1 <- model_ranked1 |> purrr::pluck(\"wflow_id\", 1) |> gsub_und()\r\n    name_model2 <- model_ranked2 |> purrr::pluck(\"wflow_id\", 1) |> gsub_und()\r\n    \r\n    workflowsets::collect_predictions(mod1) |> \r\n        dplyr::mutate(model = name_model1) |> \r\n        dplyr::bind_rows(workflowsets::collect_predictions(mod2) |> \r\n                      dplyr::mutate(model = name_model2)) |>\r\n        tidyr::nest(data = -c(model)) |> \r\n        dplyr::mutate(plots = map2(data, model, scatterplot))\r\n    \r\n}\r\n\r\naccessing_models(fit_best_mod, \r\n                 fit_intermediary_model, \r\n                 best_model, \r\n                 intermediary_model)\r\n\r\n\r\n\r\nFigure 3 - Volume observado vs volume predito usando o modelo XGBoost.Conclusão da modelagem\r\nÉ possível observar que o modelo XGBoost com pré-processamento normalizado apresentou um desempenho elevado frente aos demais modelos, apontando a sua aplicabilidade para a predição de volume de madeira de eucalipto em florestas plantadas. É digno de nota que, de forma similar ao que foi apontado por Azevedo et al. (2020), o modelo de redes neurais artificiais (MLP) apresentou um R² de ~0,96.\r\n3ª etapa - Reprodutibilidade\r\nAgora vamos para etapa que foca no uso do pacote targets. Com o targets pode ser mantido uma rotina de trabalho reprodutível que evita ao máximo repetições. O pacote “aprende” como o seu fluxo de trabalho se encaixa e pula a execução de tarefas pesadas que já estão atualizadas. Desse modo, ele executa apenas as etapas necessárias ou que foram alteradas/adicionadas. Mais informações sobre esse pacote podem ser encontradas em seu manual.\r\nE como o targets funciona?\r\nBasicamente ele depende de funções:\r\nPara todas as etapas da modelagem que foi realizada foi criada uma função que foi alocada em um script (functions.R), assim, o fluxo de trabalho é subdividido e pode rodar de forma mais independente. Lembra da pasta “Eucalipto_Volume/R” criada lá no começo? Então, dentro dessa pasta é onde está o script com as funções definidas. Esse script pode ser acessado aqui. Um exemplo de função criada para a modelagem foi a da criação da base de dados na pasta “data”:\r\n\r\n\r\nreadind_data <- function(){\r\n    \r\n    dir.create(\"data\")\r\n    \r\n    link_train <- \"https://doi.org/10.1371/journal.pone.0238703.s007\"\r\n    link_test <- \"https://doi.org/10.1371/journal.pone.0238703.s008\"\r\n    \r\n    dest_folder_train <- here::here(\"data\", \"train.xlsx\")\r\n    dest_folder_test <- here::here(\"data\", \"test.xlsx\")\r\n    \r\n    utils::download.file(link_train, \r\n                         destfile = dest_folder_train, \r\n                         mode = \"wb\") # wb se OS for Windows\r\n    utils::download.file(link_test, \r\n                         destfile = dest_folder_test, \r\n                         mode = \"wb\")\r\n    \r\n    sheet_train <- readxl::read_excel(dest_folder_train, skip = 1)\r\n    sheet_test <- readxl::read_excel(dest_folder_test, skip = 1)\r\n    \r\n    dplyr::bind_rows(sheet_train, sheet_test) |> \r\n        janitor::clean_names() |>  \r\n        dplyr::mutate(across(c(stem:rotation), \r\n                             forcats::as_factor),\r\n                      across(c(tx, d), as.integer))\r\n    \r\n}\r\n\r\n\r\n\r\nAs demais funções seguem esse mesmo estilo.\r\nUsando o targets\r\nÉ agora que o targets entra na brincadeira de vez. Para orquestrar o fluxo de trabalho primeiramente deve ser criado um script (_targets.R) alocado dentro da pasta principal (“Eucalipto_Volume”). Neste script são carregados os pacotes targets e tarchetypes (falo dele mais à frente), também é deve ser carregado o script com as funções definidas na etapa anterior e alocado na pasta “R”, a partir da função base::source() (perceba o uso do pacote here):\r\n\r\n\r\nlibrary(targets)\r\nlibrary(tarchetypes)\r\nsource(here::here(\"R\", \"functions.R\"))\r\n\r\n\r\n\r\nEm seguida, deve ser criada uma lista tendo os targets como elementos usando a função targets::tar_targets() em que o seu primeiro argumento é o nome do target a ser criado e o segundo é a função do script “functions.R” a ser usada, no seguinte esquema:\r\n\r\n\r\nlist(targets::tar_target(nome_do_target_1, funcao_a_ser_executada()), # 1\r\n     targets::tar_target(nome_do_target_2, funcao_a_ser_executada(nome_do_target_1))) # 2\r\n\r\n\r\n\r\nA função usada dentro do targets::tar_targets() geralmente requer como argumento o nome do target anterior, como no caso 2. Entretanto, targets que iniciam o fluxo de trabalho (caso 1) ou que não estão ligadas diretamente aos produtos intermediários da computação não requerem que seja passado algum argumento à sua função.\r\nNeste trabalho, a lista completa de targets definidos de acordo com as funções pode ser observada a seguir:\r\n\r\n\r\nlist(\r\n    targets::tar_target(packages, packages_used()),\r\n    targets::tar_target(dataset, readind_data()),\r\n    targets::tar_target(vol_split, split_data(dataset, 1504, 0.75, cod_volume)),\r\n    targets::tar_target(test_split, test_set_split(vol_split)),\r\n    targets::tar_target(train_split, train_set_split(vol_split)),\r\n    targets::tar_target(vol_resamples, kfold_cv(train_split, strata = cod_volume, repeats = 5)),\r\n    targets::tar_target(normalized_vol_rec, preproc_rec(train_split)),\r\n    targets::tar_target(simple_vol_rec, simple_rec(train_split)),\r\n    targets::tar_target(all_specs, def_specs()),\r\n    targets::tar_target(wflow_vol, workflow_config(all_specs, normalized_vol_rec, simple_vol_rec)),\r\n    targets::tar_target(racing_ctrl, racing_defs()),\r\n    targets::tar_target(results_vol, race_tuning(wflow_vol, vol_resamples, racing_ctrl, 1504)),\r\n    targets::tar_target(plot_rsq, plot_results(results_vol, \"rsq\")),\r\n    targets::tar_target(best_model, select_models(results_vol, \"rmse\", 1)),\r\n    targets::tar_target(intermediary_model, select_models(results_vol, \"rmse\", 10)),\r\n    targets::tar_target(fit_best_mod, fit_model(results_vol, best_model, vol_split)),\r\n    targets::tar_target(fit_intermediary_model, fit_model(results_vol, intermediary_model, vol_split)),\r\n    targets::tar_target(metrics_best, metrics_mod(fit_best_mod, best_model)),\r\n    targets::tar_target(metrics_intermediary, metrics_mod(fit_intermediary_model, intermediary_model)),\r\n    targets::tar_target(model_comparison, accessing_models(fit_best_mod,\r\n                                                           fit_intermediary_model, \r\n                                                           best_model, \r\n                                                           intermediary_model)),\r\n    tarchetypes::tar_render(report, here::here(\"report\", \"report.Rmd\"))\r\n)\r\n\r\n\r\n\r\nEssa lista possui alguns pontos dignos de destaque:\r\nO targets possui algumas ferramentas de avaliação do fluxo de trabalho estabelecido, e uma das mais legais é a função targets::tar_visnetwork. Ela permite que o pipeline seja inspecionado a partir de um fluxograma que informa se os targets estão atualizados ou não.\r\nFigure 4 - Network para inspeção do pipeline criado. Círculo em cor cinza signfica que que o target está desatualizado ou passou por algum ajuste (deve ser computado via targets::tar_make()); círculo laranja indica que há algum erro no pipeline; círculo verde indica que o target está atualizado (up-to-date).Ao fim da lista, o target tarchetypes::tar_render permite que um relatório seja gerado a partir de um arquivo “.Rmd”. Assim, foi produzido um relatório simples, usando rmardown, visando ser uma amostra do que pode ser feito a partir desse orquestramento de trabalho. Esse exemplo pode ser expandido para a produção de um artigo acadêmico ou um relatório mais complexo. Dessa forma, pode-se realizar as análises do trabalho e gerar o material final em um único pipeline. No corpo do relatório produzido, foram adicionados chamados aos 3 penúltimos objetos da lista de targets (metrics_best, metrics_intermediary, model_comparison) usando a função targets::tar_read(), em que estes objetos targets são lidos dentro do relatório. Isso é necessário para que a função tarchetypes::tar_render saiba que o relatório só poderá ser produzido quando todas as análises do pipeline forém finalizadas e estes três últimos objetos gerados.\r\nOutro ponto digno de nota é que esse processamento pode levar um tempo para ser computado. Com isso, rodá-lo novamente pode ser bem tedioso. Uma vantagem do targets é que ele memoriza os passos realizados e guarda os produtos obtidos. Dessa forma, caso seja realizada alguma alteração no pipeline, ele roda apenas as análises diretamente afetadas pela alteração feita. Por exemplo: caso seja adicionado um novo modelo na função def_specs(), que consta no arquivo functions.R, todas as análises diretamente afetadas por esse passo serão computadas novamente, porém os passos anteriores ao seu chamado não são recomputados, reduzindo o tempo e otimizando o trabalho.\r\nReproduzindo o pipeline\r\nApós realização da modelagem e orquestramento do trabalho, resta a sua publicação para que outras pessoas possam reproduzi-lo. Isso é feito de uma forma bem simples graças ao renv e ao targets. Basta clonar o repositório e salvá-lo em uma pasta principal, preferencialmente. Então, basta abrir o arquivo R Project (Eucalipto_Volume.Rproj) no RStudio e executar os seguintes comandos para instalar todos os pacotes necessários e rodar o pipeline, respectivamente:\r\n\r\n\r\nrenv::restore() # Pode demorar um pouco\r\ntargets::tar_make() # Roda o pipeline\r\n\r\n\r\n\r\nCaso você queira inspecionar o pipeline, para acompanhar o processo ou onde possa ter ocorrido possíveis falhas, basta chamar:\r\n\r\n\r\ntargets::tar_visnetwork() # Apresenta o diagrama de execução\r\n\r\n\r\n\r\nConclusão (UFA!)\r\nEssa foi uma introdução (espero que não muito cansativa) à uma das formas de utilização de modelos de machine learning com o tidymodels e de como aumentar a reprodutibilidade de trabalhos com os pacotes here, renv e targets. Confesso que esse projeto foi um trabalho bem bacana de realizar e utilizar estes pacotes se mostrou uma experiência super enriquecedora.\r\nAgradeço quem chegou até o final. Até mais!\r\nReferências\r\nAzevedo, G. B. et al. Multi-volume modeling of Eucalyptus trees using regression and artificial neural networks. Plos one, v. 15, n. 9, p. e0238703, 2020.\r\nKuhn, M., Silge, J., 2021. Tidy Modeling with R. URL: https://www.tmwr.org/\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-20-predizendo-volume-de-eucalipto-com-tidymodels-xgboost-e-targets/featured.jpg",
    "last_modified": "2021-10-20T01:10:52-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-20-xgboost-com-python-para-biometria-florestal/",
    "title": "XGBoost com Python para biometria florestal",
    "description": "Como ajustar um modelo XGBoost para predição do volume de eucalipto.",
    "author": [
      {
        "name": "Theilon Macedo",
        "url": {}
      }
    ],
    "date": "2021-10-20",
    "categories": [],
    "contents": "\r\nNo último texto\r\nque escrevi, o XGBoost foi o melhor modelo ajustado dentre todos os que\r\nforam testados. Um colega que leu o texto me solicitou um auxílio para\r\najustar este modelo usando Python, já que essa é a linguagem que ele\r\npossui maior domínio. Então decidi aproveitar e escrever um breve\r\ntutorial sobre esse ajuste.\r\nO dataset aqui utilizado é oriundo do excelente trabalho publicado\r\npublicado por Azevedo\r\net al. (2020).\r\nAprendendo com os erros\r\nXGBoost (eXtreme Gradient Boosting) é uma implementação eficiente do\r\nalgoritmo gradient boosted trees. Este algoritmo é um dos mais usados\r\natualmente dada a sua elevada capacidade preditiva, além de ter como\r\nponto forte a sua escalabilidade em diversos cenários.\r\nDe forma geral, os algoritmos gradient boosting atuam de modo a predizer\r\numa variável resposta a partir da combinação de estimativas de um set de\r\nmodelos fracos.\r\nO processo de treinamento desses modelos se dá de forma iterativa, em\r\nque são adicionada novas árvores de decisão que tem o objetivo de\r\npredizer os resíduos das árvores estabelecidas previamente, “aprendendo”\r\ncom erros das árvores ajustadas anteriormente. São chamados de gradient\r\nboosting models por usarem o algoritmo de gradiente descendente para\r\nminimizar a função de perda. Mais informações sobre esses algoritmos\r\npodem ser encontradas aqui,\r\naqui, aqui\r\ne aqui.\r\nAjustando o modelo\r\nImportando as bibliotecas necessárias:\r\n\r\nfrom xgboost import XGBRegressor\r\nfrom sklearn import metrics\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nNeste caso, o dataset já estava particionado. Porém, optei por\r\nrealizar a sua unificação de modo a seguir o padrão estipulado no texto\r\nanterior.\r\n\r\ntraining = pd.read_excel('data/train.xlsx', skiprows=1)\r\ntesting = pd.read_excel('data/test.xlsx', skiprows=1)\r\n\r\ndf = training.append(testing)\r\n\r\nX = df.filter(items=[\"DBH\", \"H\", \"TX\", \"d\"]).values.reshape(-1, 4)\r\ny = df.filter(items=[\"V\"])\r\n\r\nParticionando os dados em treino e teste com estratificação de acordo\r\ncom o volume comercial:\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, \r\n                                                    test_size=0.25, \r\n                                                    random_state=1504,\r\n                                                    stratify=df.Cod_Volume)\r\n\r\nEm seguida, defini o modelo e seus hiperparâmetros. Optei por não\r\nrealizar a tunagem destes hiperparâmetros e usar os valores já definidos\r\nno último tutorial:\r\n\r\nmodel = XGBRegressor(\r\n    objective='reg:squarederror',\r\n    n_estimators=2000,\r\n    max_depth=7,\r\n    learning_rate=0.0367,\r\n    n_jobs=10,\r\n    gamma=0.0000806,\r\n    booster=\"gbtree\",\r\n    min_child_weight=20\r\n    )\r\n\r\nFeita esta estapa, resta ajustar o modelo com os dados de treino:\r\n\r\nmodel.fit(X_train, y_train)\r\n\r\nRealizado o ajuste, agora será realizada a predição dos dados de\r\nteste para comparação e avaliação da qualidade do modelo:\r\n\r\npredicted_vol = model.predict(X_test)\r\n\r\n# Avaliação da qualidade do modelo usando o R²\r\nprint(\"Coeficiente de determinação - R²:\", metrics.r2_score(y_test, predicted_vol))\r\n\r\nO modelo apresentou um R² bastante elevado, indicando potencial de\r\nuso para a predição do volume de eucalipto.\r\nObtendo uma apresentação visual dos resultados do volume predito com o\r\nmodelo XGBoost e o volume observado em campo:\r\n\r\nmodel_results = pd.DataFrame({\"actual\": y_test[\"V\"].array, \r\n                              \"predicted\": predicted_vol}\r\n)\r\n\r\nplt.figure(figsize=(8, 7), dpi=300)\r\nplt.scatter(model_results[\"actual\"], \r\n            model_results[\"predicted\"],\r\n            s=15, \r\n            edgecolors='black', \r\n            linewidth=0.4, \r\n            alpha=0.6)\r\nplt.title(\"Volumetria Eucalipto - Modelo XGBoost\")\r\nplt.xlabel(\"Volume observado (m³)\")\r\nplt.ylabel(\"Volume predito (m³)\")\r\nz = np.polyfit(model_results[\"actual\"], \r\n               model_results[\"predicted\"], \r\n               1)\r\np = np.poly1d(z)\r\nplt.plot(model_results[\"predicted\"], \r\n         p(model_results[\"predicted\"]), \r\n         \"r--\", \r\n         color=\"black\")\r\n\r\nVolume (m³) de eucalipto observado vs\r\npredito.Como podemo concluir que o modelo generaliza bem, basta ajustá-lo\r\nusando toda a base de dados:\r\n\r\nmodel.fit(X, y)\r\n\r\nPronto! Agora basta escolher a melhor forma de salvar seu modelo seja\r\nusando pickle ou joblib (papo para outro texto).\r\nRepositório: https://github.com/TheilonMacedo/volumetria_xgboost.\r\nReferências\r\nAzevedo, G. B. et al. Multi-volume modeling of Eucalyptus trees using\r\nregression and artificial neural networks. Plos one, v. 15, n. 9,\r\np. e0238703, 2020.\r\nhttps://estatsite.com.br/2020/10/03/xgboost-em-python/\r\nJames, G. et al. An introduction to statistical learning. New York:\r\nspringer, 2013.\r\nhttps://xgboost.readthedocs.io/en/latest/\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-20-xgboost-com-python-para-biometria-florestal/scatter_vol.png",
    "last_modified": "2022-06-08T23:37:01-03:00",
    "input_file": {}
  }
]
